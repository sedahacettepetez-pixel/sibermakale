{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSW-NB15 Network Intrusion Detection - Comprehensive ML Pipeline\n",
    "\n",
    "**Authors:** Research Team  \n",
    "**Date:** 2025  \n",
    "**Version:** 1.0.0\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for network intrusion detection using the UNSW-NB15 dataset. The pipeline includes:\n",
    "\n",
    "- Data acquisition and exploratory data analysis\n",
    "- Feature engineering specific to UNSW-NB15\n",
    "- Advanced preprocessing with host-based CV splitting\n",
    "- Multiple models: LightGBM, XGBoost, CatBoost, TabTransformer\n",
    "- Ensemble learning and calibration\n",
    "- Comprehensive evaluation and visualization\n",
    "\n",
    "**Dataset:** UNSW-NB15 (Training + Test sets)\n",
    "\n",
    "**Targets:**\n",
    "- Binary: `label` (0=Normal, 1=Attack)\n",
    "- Multi-class: `attack_cat` (Normal, DoS, Exploits, Fuzzers, Generic, Reconnaissance, etc.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import (\n",
    "    RobustScaler, StandardScaler, OneHotEncoder, \n",
    "    OrdinalEncoder, LabelEncoder\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "# Gradient boosting models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "    print(\"⚠ CatBoost not available\")\n",
    "\n",
    "# Deep learning\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"⚠ PyTorch not available\")\n",
    "\n",
    "# HPO (optional)\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"⚠ Optuna not available\")\n",
    "\n",
    "# Utilities\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import custom utilities\n",
    "from utils import (\n",
    "    load_config, save_config_snapshot, ensure_directories,\n",
    "    create_data_inventory, create_eda_overview,\n",
    "    create_numeric_summary, create_categorical_summary,\n",
    "    create_target_distribution, compute_metrics,\n",
    "    plot_confusion_matrix, plot_pr_curves, plot_roc_curves,\n",
    "    plot_calibration_curve, create_feature_catalog,\n",
    "    find_optimal_threshold, create_reproducibility_manifest\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"✓ All packages imported successfully\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Pandas: {pd.__version__}, NumPy: {np.__version__}\")\n",
    "print(f\"LightGBM: {lgb.__version__}, XGBoost: {xgb.__version__}\")\n",
    "if TORCH_AVAILABLE:\n",
    "    print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Loading and Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.json')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = config['project']['seed']\n",
    "np.random.seed(SEED)\n",
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "# Ensure all directories exist\n",
    "ensure_directories(config)\n",
    "\n",
    "# Save configuration snapshot\n",
    "save_config_snapshot(\n",
    "    config,\n",
    "    os.path.join(config['output']['tables_dir'], 'config_snapshot.json')\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Configuration loaded: {config['project']['name']}\")\n",
    "print(f\"Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition (UNSW-NB15)\n",
    "\n",
    "### Option 1: Download from Kaggle\n",
    "### Option 2: Load from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Kaggle download (requires kaggle.json setup)\n",
    "# Uncomment and run if needed\n",
    "# !kaggle datasets download -d mrwellsdavid/unsw-nb15\n",
    "# !unzip -q unsw-nb15.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load from local files\n",
    "train_path = config['data']['train_path']\n",
    "test_path = config['data']['test_path']\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(train_path):\n",
    "    print(f\"⚠ Training file not found: {train_path}\")\n",
    "    print(\"Please download UNSW-NB15 dataset and place CSV files in data/ directory\")\n",
    "    raise FileNotFoundError(train_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    print(f\"⚠ Test file not found: {test_path}\")\n",
    "    print(\"Please download UNSW-NB15 dataset and place CSV files in data/ directory\")\n",
    "    raise FileNotFoundError(test_path)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# Add split column\n",
    "df_train['split'] = 'train'\n",
    "df_test['split'] = 'test'\n",
    "\n",
    "# Combine\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"Training set: {len(df_train):,} rows\")\n",
    "print(f\"Test set: {len(df_test):,} rows\")\n",
    "print(f\"Total: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Create data inventory\n",
    "inventory_df = create_data_inventory(\n",
    "    [train_path, test_path],\n",
    "    os.path.join(config['output']['tables_dir'], 'data_inventory.csv')\n",
    ")\n",
    "display(inventory_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EDA overview\n",
    "eda_overview = create_eda_overview(\n",
    "    df, config,\n",
    "    os.path.join(config['output']['tables_dir'], 'eda_overview.csv')\n",
    ")\n",
    "display(eda_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary\n",
    "numeric_summary = create_numeric_summary(\n",
    "    df, config['features']['numeric'],\n",
    "    os.path.join(config['output']['tables_dir'], 'summary_numeric.csv')\n",
    ")\n",
    "display(numeric_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical summary\n",
    "categorical_summary = create_categorical_summary(\n",
    "    df, config['features']['categorical'],\n",
    "    os.path.join(config['output']['tables_dir'], 'summary_categorical.csv')\n",
    ")\n",
    "display(categorical_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary target distribution\n",
    "target_binary = config['targets']['binary']\n",
    "binary_dist = create_target_distribution(\n",
    "    df, target_binary, 'split',\n",
    "    os.path.join(config['output']['tables_dir'], 'target_distribution_binary.csv'),\n",
    "    is_binary=True\n",
    ")\n",
    "display(binary_dist)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "df[target_binary].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
    "axes[0].set_title('Binary Label Distribution (Overall)')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Normal (0)', 'Attack (1)'], rotation=0)\n",
    "\n",
    "df.groupby('split')[target_binary].value_counts().unstack().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Binary Label Distribution by Split')\n",
    "axes[1].set_xlabel('Split')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['Normal', 'Attack'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['output']['figs_dir'], 'target_binary_dist.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class target distribution\n",
    "target_multi = config['targets']['multi']\n",
    "multi_dist = create_target_distribution(\n",
    "    df, target_multi, 'split',\n",
    "    os.path.join(config['output']['tables_dir'], 'target_distribution_multi.csv'),\n",
    "    is_binary=False\n",
    ")\n",
    "display(multi_dist.head(15))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "attack_counts = df[target_multi].value_counts()\n",
    "attack_counts.plot(kind='barh', ax=axes[0])\n",
    "axes[0].set_title('Attack Category Distribution (Overall)')\n",
    "axes[0].set_xlabel('Count')\n",
    "\n",
    "df.groupby('split')[target_multi].value_counts().unstack().T.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Attack Category Distribution by Split')\n",
    "axes[1].set_xlabel('Attack Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['Train', 'Test'])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['output']['figs_dir'], 'target_multi_dist.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Type Conversion and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track imputation\n",
    "imputation_report = []\n",
    "\n",
    "# Categorical features: convert to string\n",
    "for col in config['features']['categorical']:\n",
    "    if col in df.columns:\n",
    "        before_missing = df[col].isna().sum()\n",
    "        \n",
    "        # Handle special case: service with \"-\" values\n",
    "        if col == 'service':\n",
    "            df[col] = df[col].replace('-', '_missing_')\n",
    "        \n",
    "        # Convert to string and fill missing with '_missing_'\n",
    "        df[col] = df[col].fillna('_missing_').astype(str)\n",
    "        \n",
    "        after_missing = df[col].isna().sum()\n",
    "        imputation_report.append({\n",
    "            'column': col,\n",
    "            'strategy': 'fill(_missing_)',\n",
    "            'before_missing': before_missing,\n",
    "            'after_missing': after_missing\n",
    "        })\n",
    "\n",
    "# Numeric features: convert to float and fill with median\n",
    "for col in config['features']['numeric']:\n",
    "    if col in df.columns:\n",
    "        before_missing = df[col].isna().sum()\n",
    "        \n",
    "        # Convert to numeric, coercing errors\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Replace inf with NaN\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # Fill with median\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "        \n",
    "        after_missing = df[col].isna().sum()\n",
    "        imputation_report.append({\n",
    "            'column': col,\n",
    "            'strategy': f'median({median_val:.2f})',\n",
    "            'before_missing': before_missing,\n",
    "            'after_missing': after_missing\n",
    "        })\n",
    "\n",
    "# Save imputation report\n",
    "imputation_df = pd.DataFrame(imputation_report)\n",
    "imputation_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'imputation_report.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"✓ Data type conversion and imputation completed\")\n",
    "display(imputation_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering (UNSW-NB15 Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print(\"Performing feature engineering...\")\n",
    "\n",
    "# Bytes-related features\n",
    "df['bytes_total'] = df['sbytes'] + df['dbytes']\n",
    "df['bytes_ratio_sd'] = df['sbytes'] / (df['dbytes'] + 1)\n",
    "df['bytes_per_sec'] = df['bytes_total'] / df['dur'].clip(lower=1e-6)\n",
    "\n",
    "# Packets-related features\n",
    "df['pkts_total'] = df['spkts'] + df['dpkts']\n",
    "df['pkts_per_sec'] = df['pkts_total'] / df['dur'].clip(lower=1e-6)\n",
    "\n",
    "# Port bucketing function\n",
    "def port_to_bucket(port):\n",
    "    try:\n",
    "        port = int(port)\n",
    "        if 0 <= port <= 1023:\n",
    "            return 'well_known'\n",
    "        elif 1024 <= port <= 49151:\n",
    "            return 'registered'\n",
    "        elif 49152 <= port <= 65535:\n",
    "            return 'dynamic'\n",
    "        else:\n",
    "            return 'other'\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply port bucketing\n",
    "if 'sport' in df.columns:\n",
    "    df['sport_bucket'] = df['sport'].apply(port_to_bucket)\n",
    "else:\n",
    "    df['sport_bucket'] = 'unknown'\n",
    "\n",
    "if 'dsport' in df.columns:\n",
    "    df['dsport_bucket'] = df['dsport'].apply(port_to_bucket)\n",
    "else:\n",
    "    df['dsport_bucket'] = 'unknown'\n",
    "\n",
    "# Proto × Service interaction\n",
    "if 'proto' in df.columns and 'service' in df.columns:\n",
    "    df['proto_service'] = df['proto'].astype(str) + '_' + df['service'].astype(str)\n",
    "else:\n",
    "    df['proto_service'] = 'unknown'\n",
    "\n",
    "# Time-based features (if stime exists)\n",
    "if 'stime' in df.columns:\n",
    "    try:\n",
    "        df['hour'] = pd.to_datetime(df['stime'], unit='s', errors='coerce').dt.hour\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    except:\n",
    "        df['hour'] = 0\n",
    "        df['hour_sin'] = 0\n",
    "        df['hour_cos'] = 0\n",
    "\n",
    "# Clean inf values from engineered features\n",
    "engineered_numeric = [\n",
    "    'bytes_total', 'bytes_ratio_sd', 'bytes_per_sec',\n",
    "    'pkts_total', 'pkts_per_sec'\n",
    "]\n",
    "\n",
    "for col in engineered_numeric:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "print(\"✓ Feature engineering completed\")\n",
    "print(f\"New features created: {len(engineered_numeric) + 3}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Create feature catalog\n",
    "feature_catalog = create_feature_catalog(\n",
    "    config['features'],\n",
    "    os.path.join(config['output']['tables_dir'], 'feature_catalog.csv')\n",
    ")\n",
    "display(feature_catalog.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Strategy: Host-Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV strategy\n",
    "n_splits = config['cv']['n_splits']\n",
    "cv_strategy = config['cv']['strategy']\n",
    "\n",
    "print(f\"CV Strategy: {cv_strategy} with {n_splits} splits\")\n",
    "\n",
    "if cv_strategy == 'host':\n",
    "    # Host-based: group by (srcip, dstip)\n",
    "    if 'srcip' in df.columns and 'dstip' in df.columns:\n",
    "        df['group_key'] = df['srcip'].astype(str) + '_' + df['dstip'].astype(str)\n",
    "        \n",
    "        # Encode group_key to numeric\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le_group = LabelEncoder()\n",
    "        df['group_id'] = le_group.fit_transform(df['group_key'])\n",
    "        \n",
    "        # GroupKFold\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        \n",
    "        # Assign fold\n",
    "        df['cv_fold'] = -1\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df['group_id'])):\n",
    "            df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "        \n",
    "        print(f\"✓ Host-based CV: {df['group_key'].nunique()} unique host pairs\")\n",
    "    else:\n",
    "        print(\"⚠ srcip/dstip not found, falling back to stratified CV\")\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "        df['cv_fold'] = -1\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df[target_multi])):\n",
    "            df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "\n",
    "elif cv_strategy == 'time':\n",
    "    # Time-based: sort by stime and split into blocks\n",
    "    if 'stime' in df.columns:\n",
    "        df = df.sort_values('stime').reset_index(drop=True)\n",
    "        df['cv_fold'] = pd.qcut(df.index, q=n_splits, labels=False, duplicates='drop')\n",
    "        print(\"✓ Time-based CV\")\n",
    "    else:\n",
    "        print(\"⚠ stime not found, falling back to stratified CV\")\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "        df['cv_fold'] = -1\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df[target_multi])):\n",
    "            df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "else:\n",
    "    # Default: stratified\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    df['cv_fold'] = -1\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df[target_multi])):\n",
    "        df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "    print(\"✓ Stratified CV\")\n",
    "\n",
    "# Fold sizes\n",
    "fold_sizes = []\n",
    "for fold in range(n_splits):\n",
    "    n_val = (df['cv_fold'] == fold).sum()\n",
    "    n_train = (df['cv_fold'] != fold).sum()\n",
    "    fold_sizes.append({\n",
    "        'fold': fold,\n",
    "        'n_train': n_train,\n",
    "        'n_valid': n_val,\n",
    "        'train_pct': round(n_train / len(df) * 100, 2),\n",
    "        'valid_pct': round(n_val / len(df) * 100, 2)\n",
    "    })\n",
    "\n",
    "fold_sizes_df = pd.DataFrame(fold_sizes)\n",
    "fold_sizes_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'fold_sizes.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(fold_sizes_df)\n",
    "\n",
    "# Leakage check (host-based)\n",
    "leakage_checks = []\n",
    "if cv_strategy == 'host' and 'group_key' in df.columns:\n",
    "    has_leakage = False\n",
    "    for fold in range(n_splits):\n",
    "        train_groups = set(df[df['cv_fold'] != fold]['group_key'].unique())\n",
    "        val_groups = set(df[df['cv_fold'] == fold]['group_key'].unique())\n",
    "        overlap = train_groups & val_groups\n",
    "        if len(overlap) > 0:\n",
    "            has_leakage = True\n",
    "            break\n",
    "    \n",
    "    leakage_checks.append({\n",
    "        'check_name': 'host_group_leakage',\n",
    "        'passed': not has_leakage,\n",
    "        'detail': 'No overlap' if not has_leakage else f'{len(overlap)} groups overlap'\n",
    "    })\n",
    "else:\n",
    "    leakage_checks.append({\n",
    "        'check_name': 'host_group_leakage',\n",
    "        'passed': True,\n",
    "        'detail': 'N/A (not using host-based CV)'\n",
    "    })\n",
    "\n",
    "leakage_df = pd.DataFrame(leakage_checks)\n",
    "leakage_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'leakage_checks.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(leakage_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preprocessing Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for modeling\n",
    "categorical_features = [\n",
    "    'proto', 'service', 'state',\n",
    "    'sport_bucket', 'dsport_bucket'\n",
    "    # 'proto_service'  # High cardinality, optional\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    'dur', 'sbytes', 'dbytes', 'spkts', 'dpkts',\n",
    "    'sload', 'dload', 'sloss', 'dloss',\n",
    "    'sinpkt', 'dinpkt', 'sjit', 'djit',\n",
    "    'swin', 'dwin', 'stcpb', 'dtcpb',\n",
    "    'smeansz', 'dmeansz', 'trans_depth',\n",
    "    'res_bdy_len', 'ct_srv_src', 'ct_state_ttl',\n",
    "    'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
    "    'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd',\n",
    "    'is_sm_ips_ports',\n",
    "    # Engineered\n",
    "    'bytes_total', 'bytes_ratio_sd', 'bytes_per_sec',\n",
    "    'pkts_total', 'pkts_per_sec'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "categorical_features = [c for c in categorical_features if c in df.columns]\n",
    "numeric_features = [c for c in numeric_features if c in df.columns]\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features[:10]}...\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "        ('num', RobustScaler(), numeric_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Pipeline summary\n",
    "pipeline_summary = [\n",
    "    {'step_order': 1, 'step_name': 'OneHotEncoder', 'params_json': json.dumps({'handle_unknown': 'ignore'})},\n",
    "    {'step_order': 2, 'step_name': 'RobustScaler', 'params_json': json.dumps({})}\n",
    "]\n",
    "\n",
    "if config['imbalance']['use_smote_nc']:\n",
    "    pipeline_summary.append({\n",
    "        'step_order': 3,\n",
    "        'step_name': 'SMOTENC',\n",
    "        'params_json': json.dumps({'k_neighbors': config['imbalance']['smote_k_neighbors']})\n",
    "    })\n",
    "\n",
    "pipeline_df = pd.DataFrame(pipeline_summary)\n",
    "pipeline_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'pipeline_summary.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(pipeline_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Baseline Model (Logistic Regression) - Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test with fold 0\n",
    "print(\"Running smoke test with Logistic Regression on fold 0...\")\n",
    "\n",
    "fold = 0\n",
    "train_mask = df['cv_fold'] != fold\n",
    "val_mask = df['cv_fold'] == fold\n",
    "\n",
    "X_train = df.loc[train_mask, categorical_features + numeric_features]\n",
    "y_train = df.loc[train_mask, target_multi]\n",
    "X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "y_val = df.loc[val_mask, target_multi]\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_train_enc = le_target.fit_transform(y_train)\n",
    "y_val_enc = le_target.transform(y_val)\n",
    "\n",
    "# Preprocess\n",
    "X_train_pp = preprocessor.fit_transform(X_train)\n",
    "X_val_pp = preprocessor.transform(X_val)\n",
    "\n",
    "# Train logistic regression\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr.fit(X_train_pp, y_train_enc)\n",
    "\n",
    "# Predict\n",
    "y_pred = lr.predict(X_val_pp)\n",
    "y_proba = lr.predict_proba(X_val_pp)\n",
    "\n",
    "# Metrics\n",
    "metrics = compute_metrics(y_val_enc, y_pred, y_proba, le_target.classes_)\n",
    "print(f\"\\n✓ Smoke test results (Fold {fold}):\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "print(f\"  PR-AUC (OVR): {metrics.get('ovr_pr_auc', 0):.4f}\")\n",
    "\n",
    "# Classification report\n",
    "report_dict = classification_report(y_val_enc, y_pred, target_names=le_target.classes_, output_dict=True, zero_division=0)\n",
    "report_df = pd.DataFrame(report_dict).T\n",
    "report_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'baseline_logreg_fold0_report.csv')\n",
    ")\n",
    "display(report_df)\n",
    "\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_val_enc, y_pred, le_target.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'smoke_cm_fold0.png'),\n",
    "    title='Smoke Test - Confusion Matrix (Fold 0)'\n",
    ")\n",
    "\n",
    "# PR curve\n",
    "plot_pr_curves(\n",
    "    y_val_enc, y_proba, le_target.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'smoke_pr_fold0.png'),\n",
    "    title='Smoke Test - PR Curves (Fold 0)'\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Smoke test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "processed_path = config['data']['processed_path']\n",
    "os.makedirs(os.path.dirname(processed_path), exist_ok=True)\n",
    "\n",
    "df.to_parquet(processed_path, index=False, engine='pyarrow')\n",
    "print(f\"✓ Processed data saved to {processed_path}\")\n",
    "print(f\"File size: {os.path.getsize(processed_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Schema snapshot\n",
    "schema_data = []\n",
    "for col in df.columns:\n",
    "    if col in categorical_features:\n",
    "        role = 'categorical_feature'\n",
    "    elif col in numeric_features:\n",
    "        role = 'numeric_feature'\n",
    "    elif col in [target_binary, target_multi]:\n",
    "        role = 'target'\n",
    "    elif col in ['cv_fold', 'split']:\n",
    "        role = 'metadata'\n",
    "    else:\n",
    "        role = 'id_or_dropped'\n",
    "    \n",
    "    schema_data.append({\n",
    "        'column': col,\n",
    "        'dtype': str(df[col].dtype),\n",
    "        'role': role\n",
    "    })\n",
    "\n",
    "schema_df = pd.DataFrame(schema_data)\n",
    "schema_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'processed_schema.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(schema_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Metric Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric definitions table\n",
    "metric_defs = [\n",
    "    {'metric_name': 'macro_f1', 'definition': 'F1-score averaged across all classes (unweighted)', 'averaging': 'macro'},\n",
    "    {'metric_name': 'weighted_f1', 'definition': 'F1-score averaged across all classes (weighted by support)', 'averaging': 'weighted'},\n",
    "    {'metric_name': 'accuracy', 'definition': 'Overall classification accuracy', 'averaging': 'N/A'},\n",
    "    {'metric_name': 'ovr_pr_auc', 'definition': 'Average Precision (PR-AUC) using One-vs-Rest', 'averaging': 'ovr'},\n",
    "    {'metric_name': 'precision_macro', 'definition': 'Precision averaged across all classes', 'averaging': 'macro'},\n",
    "    {'metric_name': 'recall_macro', 'definition': 'Recall averaged across all classes', 'averaging': 'macro'},\n",
    "]\n",
    "\n",
    "metric_defs_df = pd.DataFrame(metric_defs)\n",
    "metric_defs_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'metric_definitions.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(metric_defs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Model Training and Evaluation\n",
    "\n",
    "### 13.1 LightGBM - 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM training\n",
    "print(\"=\" * 60)\n",
    "print(\"Training LightGBM with 5-Fold CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lgbm_cv_scores = []\n",
    "lgbm_preds = []\n",
    "lgbm_probas = []\n",
    "lgbm_feature_importance = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    \n",
    "    train_mask = df['cv_fold'] != fold\n",
    "    val_mask = df['cv_fold'] == fold\n",
    "    \n",
    "    X_train = df.loc[train_mask, categorical_features + numeric_features]\n",
    "    y_train = df.loc[train_mask, target_multi]\n",
    "    X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "    y_val = df.loc[val_mask, target_multi]\n",
    "    \n",
    "    # Encode target\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "    \n",
    "    # Preprocess\n",
    "    X_train_pp = preprocessor.fit_transform(X_train)\n",
    "    X_val_pp = preprocessor.transform(X_val)\n",
    "    \n",
    "    # Train LightGBM\n",
    "    lgbm_params = config['models']['lightgbm'].copy()\n",
    "    lgbm_clf = lgb.LGBMClassifier(**lgbm_params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    lgbm_clf.fit(\n",
    "        X_train_pp, y_train_enc,\n",
    "        eval_set=[(X_val_pp, y_val_enc)],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = lgbm_clf.predict(X_val_pp)\n",
    "    y_proba = lgbm_clf.predict_proba(X_val_pp)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = compute_metrics(y_val_enc, y_pred, y_proba, le.classes_)\n",
    "    lgbm_cv_scores.append({\n",
    "        'fold': fold,\n",
    "        'macro_f1': metrics['macro_f1'],\n",
    "        'ovr_pr_auc': metrics.get('ovr_pr_auc', 0),\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'train_time_sec': round(train_time, 2)\n",
    "    })\n",
    "    \n",
    "    # Store predictions\n",
    "    for i, (true, pred) in enumerate(zip(y_val_enc, y_pred)):\n",
    "        lgbm_preds.append({'fold': fold, 'y_true': true, 'y_pred': pred})\n",
    "    \n",
    "    # Store probabilities\n",
    "    for i, proba_row in enumerate(y_proba):\n",
    "        proba_dict = {'fold': fold}\n",
    "        for cls_idx, cls_name in enumerate(le.classes_):\n",
    "            proba_dict[f'p_{cls_name}'] = proba_row[cls_idx]\n",
    "        lgbm_probas.append(proba_dict)\n",
    "    \n",
    "    print(f\"Fold {fold}: Macro F1 = {metrics['macro_f1']:.4f}, PR-AUC = {metrics.get('ovr_pr_auc', 0):.4f}\")\n",
    "\n",
    "# Feature importance (from last fold)\n",
    "feature_names = (preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist() +\n",
    "                 numeric_features)\n",
    "\n",
    "if len(feature_names) == lgbm_clf.n_features_in_:\n",
    "    gain_importance = lgbm_clf.feature_importances_\n",
    "    split_importance = lgbm_clf.booster_.feature_importance(importance_type='split')\n",
    "    \n",
    "    for idx, feat in enumerate(feature_names):\n",
    "        lgbm_feature_importance.append({\n",
    "            'feature': feat,\n",
    "            'gain_importance': gain_importance[idx] if idx < len(gain_importance) else 0,\n",
    "            'split_importance': split_importance[idx] if idx < len(split_importance) else 0\n",
    "        })\n",
    "    \n",
    "    lgbm_fi_df = pd.DataFrame(lgbm_feature_importance)\n",
    "    lgbm_fi_df['rank_gain'] = lgbm_fi_df['gain_importance'].rank(ascending=False)\n",
    "    lgbm_fi_df['rank_split'] = lgbm_fi_df['split_importance'].rank(ascending=False)\n",
    "    lgbm_fi_df = lgbm_fi_df.sort_values('gain_importance', ascending=False)\n",
    "    lgbm_fi_df.to_csv(\n",
    "        os.path.join(config['output']['tables_dir'], 'lgbm_feature_importance.csv'),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "lgbm_cv_df = pd.DataFrame(lgbm_cv_scores)\n",
    "lgbm_cv_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'lgbm_cv_scores.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "lgbm_preds_df = pd.DataFrame(lgbm_preds)\n",
    "lgbm_preds_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'lgbm_preds.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "lgbm_probas_df = pd.DataFrame(lgbm_probas)\n",
    "lgbm_probas_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'lgbm_probas.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LightGBM CV Results:\")\n",
    "print(lgbm_cv_df)\n",
    "print(f\"Mean Macro F1: {lgbm_cv_df['macro_f1'].mean():.4f} ± {lgbm_cv_df['macro_f1'].std():.4f}\")\n",
    "print(f\"Mean PR-AUC: {lgbm_cv_df['ovr_pr_auc'].mean():.4f} ± {lgbm_cv_df['ovr_pr_auc'].std():.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 XGBoost - 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost training\n",
    "print(\"=\" * 60)\n",
    "print(\"Training XGBoost with 5-Fold CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_cv_scores = []\n",
    "xgb_preds = []\n",
    "xgb_probas = []\n",
    "xgb_feature_importance = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    \n",
    "    train_mask = df['cv_fold'] != fold\n",
    "    val_mask = df['cv_fold'] == fold\n",
    "    \n",
    "    X_train = df.loc[train_mask, categorical_features + numeric_features]\n",
    "    y_train = df.loc[train_mask, target_multi]\n",
    "    X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "    y_val = df.loc[val_mask, target_multi]\n",
    "    \n",
    "    # Encode target\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "    \n",
    "    # Preprocess\n",
    "    X_train_pp = preprocessor.fit_transform(X_train)\n",
    "    X_val_pp = preprocessor.transform(X_val)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_params = config['models']['xgboost'].copy()\n",
    "    xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    xgb_clf.fit(\n",
    "        X_train_pp, y_train_enc,\n",
    "        eval_set=[(X_val_pp, y_val_enc)],\n",
    "        verbose=False\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = xgb_clf.predict(X_val_pp)\n",
    "    y_proba = xgb_clf.predict_proba(X_val_pp)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = compute_metrics(y_val_enc, y_pred, y_proba, le.classes_)\n",
    "    xgb_cv_scores.append({\n",
    "        'fold': fold,\n",
    "        'macro_f1': metrics['macro_f1'],\n",
    "        'ovr_pr_auc': metrics.get('ovr_pr_auc', 0),\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'train_time_sec': round(train_time, 2)\n",
    "    })\n",
    "    \n",
    "    # Store predictions\n",
    "    for i, (true, pred) in enumerate(zip(y_val_enc, y_pred)):\n",
    "        xgb_preds.append({'fold': fold, 'y_true': true, 'y_pred': pred})\n",
    "    \n",
    "    # Store probabilities\n",
    "    for i, proba_row in enumerate(y_proba):\n",
    "        proba_dict = {'fold': fold}\n",
    "        for cls_idx, cls_name in enumerate(le.classes_):\n",
    "            proba_dict[f'p_{cls_name}'] = proba_row[cls_idx]\n",
    "        xgb_probas.append(proba_dict)\n",
    "    \n",
    "    print(f\"Fold {fold}: Macro F1 = {metrics['macro_f1']:.4f}, PR-AUC = {metrics.get('ovr_pr_auc', 0):.4f}\")\n",
    "\n",
    "# Feature importance (from last fold)\n",
    "if len(feature_names) == xgb_clf.n_features_in_:\n",
    "    importance = xgb_clf.feature_importances_\n",
    "    \n",
    "    for idx, feat in enumerate(feature_names):\n",
    "        xgb_feature_importance.append({\n",
    "            'feature': feat,\n",
    "            'importance': importance[idx] if idx < len(importance) else 0\n",
    "        })\n",
    "    \n",
    "    xgb_fi_df = pd.DataFrame(xgb_feature_importance)\n",
    "    xgb_fi_df['rank'] = xgb_fi_df['importance'].rank(ascending=False)\n",
    "    xgb_fi_df = xgb_fi_df.sort_values('importance', ascending=False)\n",
    "    xgb_fi_df.to_csv(\n",
    "        os.path.join(config['output']['tables_dir'], 'xgb_feature_importance.csv'),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "xgb_cv_df = pd.DataFrame(xgb_cv_scores)\n",
    "xgb_cv_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'xgb_cv_scores.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "xgb_preds_df = pd.DataFrame(xgb_preds)\n",
    "xgb_preds_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'xgb_preds.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "xgb_probas_df = pd.DataFrame(xgb_probas)\n",
    "xgb_probas_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'xgb_probas.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XGBoost CV Results:\")\n",
    "print(xgb_cv_df)\n",
    "print(f\"Mean Macro F1: {xgb_cv_df['macro_f1'].mean():.4f} ± {xgb_cv_df['macro_f1'].std():.4f}\")\n",
    "print(f\"Mean PR-AUC: {xgb_cv_df['ovr_pr_auc'].mean():.4f} ± {xgb_cv_df['ovr_pr_auc'].std():.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main results table\n",
    "main_results = [\n",
    "    {\n",
    "        'model': 'LightGBM',\n",
    "        'macro_f1_mean': lgbm_cv_df['macro_f1'].mean(),\n",
    "        'macro_f1_std': lgbm_cv_df['macro_f1'].std(),\n",
    "        'ovr_pr_auc_mean': lgbm_cv_df['ovr_pr_auc'].mean(),\n",
    "        'accuracy_mean': lgbm_cv_df['accuracy'].mean(),\n",
    "        'train_time_sec_mean': lgbm_cv_df['train_time_sec'].mean()\n",
    "    },\n",
    "    {\n",
    "        'model': 'XGBoost',\n",
    "        'macro_f1_mean': xgb_cv_df['macro_f1'].mean(),\n",
    "        'macro_f1_std': xgb_cv_df['macro_f1'].std(),\n",
    "        'ovr_pr_auc_mean': xgb_cv_df['ovr_pr_auc'].mean(),\n",
    "        'accuracy_mean': xgb_cv_df['accuracy'].mean(),\n",
    "        'train_time_sec_mean': xgb_cv_df['train_time_sec'].mean()\n",
    "    }\n",
    "]\n",
    "\n",
    "main_results_df = pd.DataFrame(main_results)\n",
    "main_results_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'main_results.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(main_results_df)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4 Visualization - PR and ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curves for LightGBM (using fold 0)\n",
    "fold = 0\n",
    "train_mask = df['cv_fold'] != fold\n",
    "val_mask = df['cv_fold'] == fold\n",
    "\n",
    "X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "y_val = df.loc[val_mask, target_multi]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df[target_multi])\n",
    "y_val_enc = le.transform(y_val)\n",
    "\n",
    "# Get probabilities from saved results\n",
    "lgbm_probas_fold0 = lgbm_probas_df[lgbm_probas_df['fold'] == fold]\n",
    "proba_cols = [c for c in lgbm_probas_fold0.columns if c.startswith('p_')]\n",
    "y_proba_lgbm = lgbm_probas_fold0[proba_cols].values\n",
    "\n",
    "# Plot PR curves\n",
    "plot_pr_curves(\n",
    "    y_val_enc, y_proba_lgbm, le.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'pr_curve_lgbm_ovr.png'),\n",
    "    title='LightGBM - Precision-Recall Curves (Fold 0, OVR)'\n",
    ")\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curves(\n",
    "    y_val_enc, y_proba_lgbm, le.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'roc_curve_lgbm_ovr.png'),\n",
    "    title='LightGBM - ROC Curves (Fold 0, OVR)'\n",
    ")\n",
    "\n",
    "# Confusion matrix\n",
    "lgbm_preds_fold0 = lgbm_preds_df[lgbm_preds_df['fold'] == fold]\n",
    "y_pred_lgbm = lgbm_preds_fold0['y_pred'].values\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    y_val_enc, y_pred_lgbm, le.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'cm_lgbm_fold0.png'),\n",
    "    title='LightGBM - Confusion Matrix (Fold 0)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5 Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 20 features for LightGBM\n",
    "if len(lgbm_feature_importance) > 0:\n",
    "    lgbm_fi_df = pd.read_csv(\n",
    "        os.path.join(config['output']['tables_dir'], 'lgbm_feature_importance.csv')\n",
    "    )\n",
    "    \n",
    "    top_features = lgbm_fi_df.nlargest(20, 'gain_importance')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['gain_importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Gain Importance')\n",
    "    plt.title('LightGBM - Top 20 Features by Gain')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(config['output']['figs_dir'], 'feature_importance_lgbm.png'),\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (LightGBM):\")\n",
    "    display(lgbm_fi_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Reproducibility Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reproducibility manifest\n",
    "manifest_df = create_reproducibility_manifest(\n",
    "    os.path.join(config['output']['tables_dir'], 'reproducibility_manifest.csv')\n",
    ")\n",
    "display(manifest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNSW-NB15 ANALYSIS - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset: {len(df):,} samples, {len(df.columns)} features\")\n",
    "print(f\"Target classes: {len(le_target.classes_)} ({', '.join(le_target.classes_)})\")\n",
    "print(f\"CV Strategy: {cv_strategy} with {n_splits} folds\")\n",
    "print(f\"\\nModels trained: LightGBM, XGBoost\")\n",
    "print(f\"\\nBest model by Macro F1: \", end=\"\")\n",
    "\n",
    "best_f1 = main_results_df.loc[main_results_df['macro_f1_mean'].idxmax()]\n",
    "print(f\"{best_f1['model']} (F1={best_f1['macro_f1_mean']:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ All results saved to: {config['output']['artifacts_dir']}\")\n",
    "print(f\"  - Tables: {config['output']['tables_dir']}\")\n",
    "print(f\"  - Figures: {config['output']['figs_dir']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps (Optional Extensions)\n",
    "\n",
    "1. **TabTransformer (PyTorch)**: Implement deep learning model for tabular data\n",
    "2. **Hyperparameter Optimization (Optuna)**: Automated HPO for better performance\n",
    "3. **Ensemble Learning**: Combine predictions from multiple models\n",
    "4. **Calibration**: Isotonic/Platt calibration for probability estimates\n",
    "5. **Ablation Studies**: Test impact of SMOTE, focal loss, etc.\n",
    "6. **SHAP Analysis**: Detailed explainability with SHAP values\n",
    "7. **Cross-Dataset Evaluation**: Test on CIC-IDS2017 or other datasets\n",
    "\n",
    "To implement these extensions, uncomment and run the corresponding sections below or create new notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for TabTransformer implementation\n",
    "# See separate notebook: tabtransformer_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for Optuna HPO\n",
    "# See separate notebook: hyperparameter_optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook**\n",
    "\n",
    "For questions or issues, please refer to the project README or contact the research team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
