{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSW-NB15 Network Intrusion Detection - Comprehensive ML Pipeline\n",
    "\n",
    "**Authors:** Research Team  \n",
    "**Date:** 2025  \n",
    "**Version:** 1.0.0\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for network intrusion detection using the UNSW-NB15 dataset. The pipeline includes:\n",
    "\n",
    "- Data acquisition and exploratory data analysis\n",
    "- Feature engineering specific to UNSW-NB15\n",
    "- Advanced preprocessing with host-based CV splitting\n",
    "- Multiple models: LightGBM, XGBoost, CatBoost, TabTransformer\n",
    "- Ensemble learning and calibration\n",
    "- Comprehensive evaluation and visualization\n",
    "\n",
    "**Dataset:** UNSW-NB15 (Training + Test sets)\n",
    "\n",
    "**Targets:**\n",
    "- Binary: `label` (0=Normal, 1=Attack)\n",
    "- Multi-class: `attack_cat` (Normal, DoS, Exploits, Fuzzers, Generic, Reconnaissance, etc.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core libraries\nimport os\nimport sys\nimport json\nimport time\nimport warnings\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.preprocessing import (\n    RobustScaler, StandardScaler, OneHotEncoder, \n    OrdinalEncoder, LabelEncoder\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    f1_score, precision_score, recall_score, accuracy_score,\n    roc_auc_score, average_precision_score\n)\n\n# Imbalanced learning\nfrom imblearn.over_sampling import SMOTENC\n\n# Gradient boosting models\nimport lightgbm as lgb\nimport xgboost as xgb\ntry:\n    import catboost as cb\n    CATBOOST_AVAILABLE = True\nexcept ImportError:\n    CATBOOST_AVAILABLE = False\n    print(\"⚠ CatBoost not available\")\n\n# Deep learning\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader, TensorDataset\n    TORCH_AVAILABLE = True\nexcept ImportError:\n    TORCH_AVAILABLE = False\n    print(\"⚠ PyTorch not available\")\n\n# HPO (optional)\ntry:\n    import optuna\n    OPTUNA_AVAILABLE = True\nexcept ImportError:\n    OPTUNA_AVAILABLE = False\n    print(\"⚠ Optuna not available\")\n\n# Utilities\nfrom tqdm.auto import tqdm\n\n# Import custom utilities\nfrom utils import (\n    load_config, save_config_snapshot, ensure_directories,\n    create_data_inventory, create_eda_overview,\n    create_numeric_summary, create_categorical_summary,\n    create_target_distribution, compute_metrics,\n    plot_confusion_matrix, plot_pr_curves, plot_roc_curves,\n    plot_calibration_curve, create_feature_catalog,\n    find_optimal_threshold, create_reproducibility_manifest,\n    plot_numeric_distributions, plot_correlation_heatmap,\n    plot_feature_importance_comparison, plot_per_class_metrics,\n    plot_training_time_comparison, plot_model_comparison_radar,\n    plot_class_distribution_per_fold, plot_categorical_distribution,\n    plot_metric_progression, create_per_class_metrics_table,\n    plot_boxplot_comparison\n)\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.dpi'] = 100\n\nprint(\"✓ All packages imported successfully\")\nprint(f\"Python version: {sys.version.split()[0]}\")\nprint(f\"Pandas: {pd.__version__}, NumPy: {np.__version__}\")\nprint(f\"LightGBM: {lgb.__version__}, XGBoost: {xgb.__version__}\")\nif TORCH_AVAILABLE:\n    print(f\"PyTorch: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Loading and Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.json')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = config['project']['seed']\n",
    "np.random.seed(SEED)\n",
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "# Ensure all directories exist\n",
    "ensure_directories(config)\n",
    "\n",
    "# Save configuration snapshot\n",
    "save_config_snapshot(\n",
    "    config,\n",
    "    os.path.join(config['output']['tables_dir'], 'config_snapshot.json')\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Configuration loaded: {config['project']['name']}\")\n",
    "print(f\"Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition (UNSW-NB15)\n",
    "\n",
    "### Option 1: Download from Kaggle\n",
    "### Option 2: Load from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Kaggle download (requires kaggle.json setup)\n",
    "# Uncomment and run if needed\n",
    "# !kaggle datasets download -d mrwellsdavid/unsw-nb15\n",
    "# !unzip -q unsw-nb15.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load from local files\n",
    "train_path = config['data']['train_path']\n",
    "test_path = config['data']['test_path']\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(train_path):\n",
    "    print(f\"⚠ Training file not found: {train_path}\")\n",
    "    print(\"Please download UNSW-NB15 dataset and place CSV files in data/ directory\")\n",
    "    raise FileNotFoundError(train_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    print(f\"⚠ Test file not found: {test_path}\")\n",
    "    print(\"Please download UNSW-NB15 dataset and place CSV files in data/ directory\")\n",
    "    raise FileNotFoundError(test_path)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# Add split column\n",
    "df_train['split'] = 'train'\n",
    "df_test['split'] = 'test'\n",
    "\n",
    "# Combine\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully\")\n",
    "print(f\"Training set: {len(df_train):,} rows\")\n",
    "print(f\"Test set: {len(df_test):,} rows\")\n",
    "print(f\"Total: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Create data inventory\n",
    "inventory_df = create_data_inventory(\n",
    "    [train_path, test_path],\n",
    "    os.path.join(config['output']['tables_dir'], 'data_inventory.csv')\n",
    ")\n",
    "display(inventory_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EDA overview\n",
    "eda_overview = create_eda_overview(\n",
    "    df, config,\n",
    "    os.path.join(config['output']['tables_dir'], 'eda_overview.csv')\n",
    ")\n",
    "display(eda_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary\n",
    "numeric_summary = create_numeric_summary(\n",
    "    df, config['features']['numeric'],\n",
    "    os.path.join(config['output']['tables_dir'], 'summary_numeric.csv')\n",
    ")\n",
    "display(numeric_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical summary\n",
    "categorical_summary = create_categorical_summary(\n",
    "    df, config['features']['categorical'],\n",
    "    os.path.join(config['output']['tables_dir'], 'summary_categorical.csv')\n",
    ")\n",
    "display(categorical_summary)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Plot numeric feature distributions\nplot_numeric_distributions(df, config['features']['numeric'][:20],\n                           config['output']['figs_dir'], n_cols=5)\n\nprint(\"✓ Numeric distributions plotted\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.1 Numeric Feature Distributions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary target distribution\n",
    "target_binary = config['targets']['binary']\n",
    "binary_dist = create_target_distribution(\n",
    "    df, target_binary, 'split',\n",
    "    os.path.join(config['output']['tables_dir'], 'target_distribution_binary.csv'),\n",
    "    is_binary=True\n",
    ")\n",
    "display(binary_dist)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "df[target_binary].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
    "axes[0].set_title('Binary Label Distribution (Overall)')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Normal (0)', 'Attack (1)'], rotation=0)\n",
    "\n",
    "df.groupby('split')[target_binary].value_counts().unstack().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Binary Label Distribution by Split')\n",
    "axes[1].set_xlabel('Split')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['Normal', 'Attack'])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['output']['figs_dir'], 'target_binary_dist.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class target distribution\n",
    "target_multi = config['targets']['multi']\n",
    "multi_dist = create_target_distribution(\n",
    "    df, target_multi, 'split',\n",
    "    os.path.join(config['output']['tables_dir'], 'target_distribution_multi.csv'),\n",
    "    is_binary=False\n",
    ")\n",
    "display(multi_dist.head(15))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "attack_counts = df[target_multi].value_counts()\n",
    "attack_counts.plot(kind='barh', ax=axes[0])\n",
    "axes[0].set_title('Attack Category Distribution (Overall)')\n",
    "axes[0].set_xlabel('Count')\n",
    "\n",
    "df.groupby('split')[target_multi].value_counts().unstack().T.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Attack Category Distribution by Split')\n",
    "axes[1].set_xlabel('Attack Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend(['Train', 'Test'])\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(config['output']['figs_dir'], 'target_multi_dist.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Type Conversion and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# ADVANCED FEATURE ENGINEERING (UNSW-NB15 Specific)\n# ============================================================================\n\nprint(\"Performing advanced feature engineering...\")\nprint(\"=\"*80)\n\n# 1. BYTES-RELATED FEATURES\nprint(\"\\n[1/6] Creating bytes-related features...\")\ndf['bytes_total'] = df['sbytes'] + df['dbytes']\ndf['bytes_ratio_sd'] = df['sbytes'] / (df['dbytes'] + 1)\ndf['bytes_ratio_ds'] = df['dbytes'] / (df['sbytes'] + 1)\ndf['bytes_per_sec'] = df['bytes_total'] / df['dur'].clip(lower=1e-6)\ndf['bytes_per_pkt'] = df['bytes_total'] / (df['spkts'] + df['dpkts'] + 1)\n\n# Mean sizes\ndf['bytes_mean_size'] = (df['smeansz'] + df['dmeansz']) / 2\ndf['bytes_size_diff'] = np.abs(df['smeansz'] - df['dmeansz'])\n\n# 2. PACKETS-RELATED FEATURES\nprint(\"[2/6] Creating packet-related features...\")\ndf['pkts_total'] = df['spkts'] + df['dpkts']\ndf['pkts_ratio_sd'] = df['spkts'] / (df['dpkts'] + 1)\ndf['pkts_per_sec'] = df['pkts_total'] / df['dur'].clip(lower=1e-6)\ndf['pkts_density'] = df['pkts_total'] / (df['bytes_total'] + 1)  # packets per byte\n\n# Packet rates\ndf['spkts_rate'] = df['spkts'] / df['dur'].clip(lower=1e-6)\ndf['dpkts_rate'] = df['dpkts'] / df['dur'].clip(lower=1e-6)\n\n# 3. LOAD AND JITTER FEATURES\nprint(\"[3/6] Creating load and jitter features...\")\ndf['load_total'] = df['sload'] + df['dload']\ndf['load_ratio'] = df['sload'] / (df['dload'] + 1)\ndf['jitter_total'] = df['sjit'] + df['djit']\ndf['jitter_ratio'] = df['sjit'] / (df['djit'] + 1)\n\n# Loss features\ndf['loss_total'] = df['sloss'] + df['dloss']\ndf['loss_ratio'] = df['sloss'] / (df['dloss'] + 1)\n\n# 4. STATE-TTL RELATIONSHIP FEATURES\nprint(\"[4/6] Creating state-TTL interaction features...\")\nif 'state' in df.columns and 'ct_state_ttl' in df.columns:\n    # State-TTL intensity\n    df['state_ttl_intensity'] = df.groupby('state')['ct_state_ttl'].transform('mean')\n    df['state_ttl_deviation'] = df['ct_state_ttl'] - df['state_ttl_intensity']\nelse:\n    df['state_ttl_intensity'] = 0\n    df['state_ttl_deviation'] = 0\n\n# Connection time features\nif 'ct_srv_src' in df.columns and 'ct_dst_ltm' in df.columns:\n    df['ct_ratio_srv_dst'] = df['ct_srv_src'] / (df['ct_dst_ltm'] + 1)\n\n# 5. PORT BUCKETING AND ANALYSIS\nprint(\"[5/6] Creating advanced port features...\")\n\ndef port_to_bucket(port):\n    \"\"\"Categorize ports into well-known, registered, dynamic, or unknown.\"\"\"\n    try:\n        port = int(port)\n        if 0 <= port <= 1023:\n            return 'well_known'\n        elif 1024 <= port <= 49151:\n            return 'registered'\n        elif 49152 <= port <= 65535:\n            return 'dynamic'\n        else:\n            return 'other'\n    except:\n        return 'unknown'\n\ndef port_to_service_type(port):\n    \"\"\"Map common ports to service types.\"\"\"\n    try:\n        port = int(port)\n        # HTTP/HTTPS\n        if port in [80, 443, 8080, 8443]:\n            return 'web'\n        # Email\n        elif port in [25, 110, 143, 465, 587, 993, 995]:\n            return 'email'\n        # DNS\n        elif port == 53:\n            return 'dns'\n        # FTP\n        elif port in [20, 21]:\n            return 'ftp'\n        # SSH/Telnet\n        elif port in [22, 23]:\n            return 'remote'\n        # Database\n        elif port in [3306, 5432, 1433, 1521, 27017]:\n            return 'database'\n        else:\n            return 'other'\n    except:\n        return 'unknown'\n\n# Apply port bucketing\nif 'sport' in df.columns:\n    df['sport_bucket'] = df['sport'].apply(port_to_bucket)\n    df['sport_service_type'] = df['sport'].apply(port_to_service_type)\nelse:\n    df['sport_bucket'] = 'unknown'\n    df['sport_service_type'] = 'unknown'\n\nif 'dsport' in df.columns:\n    df['dsport_bucket'] = df['dsport'].apply(port_to_bucket)\n    df['dsport_service_type'] = df['dsport'].apply(port_to_service_type)\nelse:\n    df['dsport_bucket'] = 'unknown'\n    df['dsport_service_type'] = 'unknown'\n\n# Port pair indicator\ndf['same_port_bucket'] = (df['sport_bucket'] == df['dsport_bucket']).astype(int)\n\n# 6. PROTOCOL × SERVICE × STATE INTERACTIONS\nprint(\"[6/6] Creating interaction features...\")\nif 'proto' in df.columns and 'service' in df.columns:\n    df['proto_service'] = df['proto'].astype(str) + '_' + df['service'].astype(str)\nelse:\n    df['proto_service'] = 'unknown'\n\nif 'proto' in df.columns and 'state' in df.columns:\n    df['proto_state'] = df['proto'].astype(str) + '_' + df['state'].astype(str)\nelse:\n    df['proto_state'] = 'unknown'\n\n# IP address features\nif 'is_sm_ips_ports' in df.columns:\n    df['ip_port_same'] = df['is_sm_ips_ports']\nelse:\n    df['ip_port_same'] = 0\n\n# Time-based features (if stime exists)\nif 'stime' in df.columns:\n    try:\n        df['stime_dt'] = pd.to_datetime(df['stime'], unit='s', errors='coerce')\n        df['hour'] = df['stime_dt'].dt.hour\n        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n        df['is_business_hour'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)\n        df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n    except:\n        df['hour'] = 0\n        df['hour_sin'] = 0\n        df['hour_cos'] = 0\n        df['is_business_hour'] = 0\n        df['is_night'] = 0\n\n# Clean inf values from all engineered features\nengineered_numeric = [\n    'bytes_total', 'bytes_ratio_sd', 'bytes_ratio_ds', 'bytes_per_sec', 'bytes_per_pkt',\n    'bytes_mean_size', 'bytes_size_diff',\n    'pkts_total', 'pkts_ratio_sd', 'pkts_per_sec', 'pkts_density',\n    'spkts_rate', 'dpkts_rate',\n    'load_total', 'load_ratio', 'jitter_total', 'jitter_ratio',\n    'loss_total', 'loss_ratio',\n    'state_ttl_intensity', 'state_ttl_deviation', 'ct_ratio_srv_dst'\n]\n\nfor col in engineered_numeric:\n    if col in df.columns:\n        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n        df[col] = df[col].fillna(0)\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"✓ Advanced feature engineering completed!\")\nprint(f\"  Original features: ~45\")\nprint(f\"  Engineered numeric features: {len(engineered_numeric)}\")\nprint(f\"  Engineered categorical features: 5 (port buckets, service types, interactions)\")\nprint(f\"  Total columns now: {len(df.columns)}\")\nprint(\"=\"*80)\n\n# Create comprehensive feature catalog\nfeature_catalog_data = []\n\n# Original categorical\nfor feat in config['features']['categorical']:\n    feature_catalog_data.append({\n        'feature': feat,\n        'type': 'categorical',\n        'source': 'original',\n        'description': f'Original categorical feature from UNSW-NB15'\n    })\n\n# Original numeric\nfor feat in config['features']['numeric']:\n    feature_catalog_data.append({\n        'feature': feat,\n        'type': 'numeric',\n        'source': 'original',\n        'description': f'Original numeric feature from UNSW-NB15'\n    })\n\n# Engineered features\nengineered_catalog = {\n    # Bytes features\n    'bytes_total': 'Total bytes (source + destination)',\n    'bytes_ratio_sd': 'Bytes ratio (source/destination)',\n    'bytes_ratio_ds': 'Bytes ratio (destination/source)',\n    'bytes_per_sec': 'Bytes transfer rate per second',\n    'bytes_per_pkt': 'Average bytes per packet',\n    'bytes_mean_size': 'Mean of source and destination mean sizes',\n    'bytes_size_diff': 'Absolute difference in mean sizes',\n    \n    # Packet features\n    'pkts_total': 'Total packets (source + destination)',\n    'pkts_ratio_sd': 'Packet ratio (source/destination)',\n    'pkts_per_sec': 'Packet rate per second',\n    'pkts_density': 'Packets per byte ratio',\n    'spkts_rate': 'Source packet rate',\n    'dpkts_rate': 'Destination packet rate',\n    \n    # Load and jitter\n    'load_total': 'Total load (source + destination)',\n    'load_ratio': 'Load ratio (source/destination)',\n    'jitter_total': 'Total jitter (source + destination)',\n    'jitter_ratio': 'Jitter ratio (source/destination)',\n    'loss_total': 'Total packet loss',\n    'loss_ratio': 'Loss ratio (source/destination)',\n    \n    # State-TTL\n    'state_ttl_intensity': 'Mean TTL for connection state',\n    'state_ttl_deviation': 'Deviation from state mean TTL',\n    'ct_ratio_srv_dst': 'Connection time ratio (service/destination)',\n    \n    # Port features\n    'sport_bucket': 'Source port category (well-known/registered/dynamic)',\n    'dsport_bucket': 'Destination port category',\n    'sport_service_type': 'Source port service type (web/email/dns/etc)',\n    'dsport_service_type': 'Destination port service type',\n    'same_port_bucket': 'Binary: same source/destination port bucket',\n    \n    # Interactions\n    'proto_service': 'Protocol × Service interaction',\n    'proto_state': 'Protocol × State interaction',\n    \n    # Time features\n    'hour': 'Hour of day from stime',\n    'hour_sin': 'Sine encoding of hour (cyclical)',\n    'hour_cos': 'Cosine encoding of hour (cyclical)',\n    'is_business_hour': 'Binary: during business hours (9-17)',\n    'is_night': 'Binary: nighttime (22-6)'\n}\n\nfor feat, desc in engineered_catalog.items():\n    if feat in df.columns:\n        feat_type = 'categorical' if feat in ['sport_bucket', 'dsport_bucket', 'sport_service_type', \n                                               'dsport_service_type', 'proto_service', 'proto_state'] else 'numeric'\n        feature_catalog_data.append({\n            'feature': feat,\n            'type': feat_type,\n            'source': 'engineered',\n            'description': desc\n        })\n\nfeature_catalog_df = pd.DataFrame(feature_catalog_data)\nfeature_catalog_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'feature_catalog_comprehensive.csv'),\n    index=False\n)\n\nprint(f\"\\n✓ Comprehensive feature catalog saved\")\ndisplay(feature_catalog_df[feature_catalog_df['source'] == 'engineered'].head(20))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering (UNSW-NB15 Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "print(\"Performing feature engineering...\")\n",
    "\n",
    "# Bytes-related features\n",
    "df['bytes_total'] = df['sbytes'] + df['dbytes']\n",
    "df['bytes_ratio_sd'] = df['sbytes'] / (df['dbytes'] + 1)\n",
    "df['bytes_per_sec'] = df['bytes_total'] / df['dur'].clip(lower=1e-6)\n",
    "\n",
    "# Packets-related features\n",
    "df['pkts_total'] = df['spkts'] + df['dpkts']\n",
    "df['pkts_per_sec'] = df['pkts_total'] / df['dur'].clip(lower=1e-6)\n",
    "\n",
    "# Port bucketing function\n",
    "def port_to_bucket(port):\n",
    "    try:\n",
    "        port = int(port)\n",
    "        if 0 <= port <= 1023:\n",
    "            return 'well_known'\n",
    "        elif 1024 <= port <= 49151:\n",
    "            return 'registered'\n",
    "        elif 49152 <= port <= 65535:\n",
    "            return 'dynamic'\n",
    "        else:\n",
    "            return 'other'\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply port bucketing\n",
    "if 'sport' in df.columns:\n",
    "    df['sport_bucket'] = df['sport'].apply(port_to_bucket)\n",
    "else:\n",
    "    df['sport_bucket'] = 'unknown'\n",
    "\n",
    "if 'dsport' in df.columns:\n",
    "    df['dsport_bucket'] = df['dsport'].apply(port_to_bucket)\n",
    "else:\n",
    "    df['dsport_bucket'] = 'unknown'\n",
    "\n",
    "# Proto × Service interaction\n",
    "if 'proto' in df.columns and 'service' in df.columns:\n",
    "    df['proto_service'] = df['proto'].astype(str) + '_' + df['service'].astype(str)\n",
    "else:\n",
    "    df['proto_service'] = 'unknown'\n",
    "\n",
    "# Time-based features (if stime exists)\n",
    "if 'stime' in df.columns:\n",
    "    try:\n",
    "        df['hour'] = pd.to_datetime(df['stime'], unit='s', errors='coerce').dt.hour\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    except:\n",
    "        df['hour'] = 0\n",
    "        df['hour_sin'] = 0\n",
    "        df['hour_cos'] = 0\n",
    "\n",
    "# Clean inf values from engineered features\n",
    "engineered_numeric = [\n",
    "    'bytes_total', 'bytes_ratio_sd', 'bytes_per_sec',\n",
    "    'pkts_total', 'pkts_per_sec'\n",
    "]\n",
    "\n",
    "for col in engineered_numeric:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "print(\"✓ Feature engineering completed\")\n",
    "print(f\"New features created: {len(engineered_numeric) + 3}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Create feature catalog\n",
    "feature_catalog = create_feature_catalog(\n",
    "    config['features'],\n",
    "    os.path.join(config['output']['tables_dir'], 'feature_catalog.csv')\n",
    ")\n",
    "display(feature_catalog.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Strategy: Host-Based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV strategy\n",
    "n_splits = config['cv']['n_splits']\n",
    "cv_strategy = config['cv']['strategy']\n",
    "\n",
    "print(f\"CV Strategy: {cv_strategy} with {n_splits} splits\")\n",
    "\n",
    "if cv_strategy == 'host':\n",
    "    # Host-based: group by (srcip, dstip)\n",
    "    if 'srcip' in df.columns and 'dstip' in df.columns:\n",
    "        df['group_key'] = df['srcip'].astype(str) + '_' + df['dstip'].astype(str)\n",
    "        \n",
    "        # Encode group_key to numeric\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le_group = LabelEncoder()\n",
    "        df['group_id'] = le_group.fit_transform(df['group_key'])\n",
    "        \n",
    "        # GroupKFold\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        \n",
    "        # Assign fold\n",
    "        df['cv_fold'] = -1\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(df, groups=df['group_id'])):\n",
    "            df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "        \n",
    "        print(f\"✓ Host-based CV: {df['group_key'].nunique()} unique host pairs\")\n",
    "    else:\n",
    "        print(\"⚠ srcip/dstip not found, falling back to stratified CV\")\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "        df['cv_fold'] = -1\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df[target_multi])):\n",
    "            df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "\n",
    "elif cv_strategy == 'time':\n",
    "    # Time-based: sort by stime and split into blocks\n",
    "    if 'stime' in df.columns:\n",
    "        df = df.sort_values('stime').reset_index(drop=True)\n",
    "        df['cv_fold'] = pd.qcut(df.index, q=n_splits, labels=False, duplicates='drop')\n",
    "        print(\"✓ Time-based CV\")\n",
    "    else:\n",
    "        print(\"⚠ stime not found, falling back to stratified CV\")\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "        df['cv_fold'] = -1\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df[target_multi])):\n",
    "            df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "else:\n",
    "    # Default: stratified\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    df['cv_fold'] = -1\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(df, df[target_multi])):\n",
    "        df.loc[val_idx, 'cv_fold'] = fold_idx\n",
    "    print(\"✓ Stratified CV\")\n",
    "\n",
    "# Fold sizes\n",
    "fold_sizes = []\n",
    "for fold in range(n_splits):\n",
    "    n_val = (df['cv_fold'] == fold).sum()\n",
    "    n_train = (df['cv_fold'] != fold).sum()\n",
    "    fold_sizes.append({\n",
    "        'fold': fold,\n",
    "        'n_train': n_train,\n",
    "        'n_valid': n_val,\n",
    "        'train_pct': round(n_train / len(df) * 100, 2),\n",
    "        'valid_pct': round(n_val / len(df) * 100, 2)\n",
    "    })\n",
    "\n",
    "fold_sizes_df = pd.DataFrame(fold_sizes)\n",
    "fold_sizes_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'fold_sizes.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(fold_sizes_df)\n",
    "\n",
    "# Leakage check (host-based)\n",
    "leakage_checks = []\n",
    "if cv_strategy == 'host' and 'group_key' in df.columns:\n",
    "    has_leakage = False\n",
    "    for fold in range(n_splits):\n",
    "        train_groups = set(df[df['cv_fold'] != fold]['group_key'].unique())\n",
    "        val_groups = set(df[df['cv_fold'] == fold]['group_key'].unique())\n",
    "        overlap = train_groups & val_groups\n",
    "        if len(overlap) > 0:\n",
    "            has_leakage = True\n",
    "            break\n",
    "    \n",
    "    leakage_checks.append({\n",
    "        'check_name': 'host_group_leakage',\n",
    "        'passed': not has_leakage,\n",
    "        'detail': 'No overlap' if not has_leakage else f'{len(overlap)} groups overlap'\n",
    "    })\n",
    "else:\n",
    "    leakage_checks.append({\n",
    "        'check_name': 'host_group_leakage',\n",
    "        'passed': True,\n",
    "        'detail': 'N/A (not using host-based CV)'\n",
    "    })\n",
    "\n",
    "leakage_df = pd.DataFrame(leakage_checks)\n",
    "leakage_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'leakage_checks.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(leakage_df)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# SMOTENC - SYNTHETIC MINORITY OVERSAMPLING FOR IMBALANCED DATA\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"SMOTENC - IMBALANCED DATA HANDLING\")\nprint(\"=\"*80)\n\n# Check class distribution before SMOTENC\nprint(\"\\nClass distribution BEFORE SMOTENC:\")\nclass_dist_before = df[target_multi].value_counts()\nprint(class_dist_before)\n\n# Prepare data for SMOTENC demonstration (using fold 0 as example)\nfold = 0\ntrain_mask = df['cv_fold'] != fold\n\nX_train_demo = df.loc[train_mask, categorical_features + numeric_features]\ny_train_demo = df.loc[train_mask, target_multi]\n\n# Encode target for SMOTENC\nle_smote = LabelEncoder()\ny_train_demo_enc = le_smote.fit_transform(y_train_demo)\n\n# Identify categorical feature indices after combining features\ncategorical_feature_indices = list(range(len(categorical_features)))\n\nprint(f\"\\nTotal features: {len(categorical_features) + len(numeric_features)}\")\nprint(f\"Categorical features: {len(categorical_features)}\")\nprint(f\"Numeric features: {len(numeric_features)}\")\n\n# Preprocess BEFORE SMOTENC (OneHotEncode categoricals, scale numerics)\nX_train_demo_pp = preprocessor.fit_transform(X_train_demo)\n\nprint(f\"\\nAfter preprocessing: {X_train_demo_pp.shape}\")\n\n# Class distribution before\nunique, counts = np.unique(y_train_demo_enc, return_counts=True)\nclass_counts_before = dict(zip(le_smote.classes_[unique], counts))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"APPLYING SMOTENC...\")\nprint(\"=\"*80)\n\n# Apply SMOTENC only if enabled in config\nuse_smotenc = config['imbalance'].get('use_smote_nc', False)\nk_neighbors = config['imbalance'].get('smote_k_neighbors', 5)\n\nif use_smotenc and len(categorical_features) > 0:\n    # Note: SMOTENC works on UNENCODED data, so we need original features\n    # We'll apply it during training loop instead\n    print(\"⚠ SMOTENC will be applied during model training (per fold)\")\n    print(f\"  k_neighbors: {k_neighbors}\")\n    print(\"  Categorical indices: First {len(categorical_features)} features\")\n    \n    # Save SMOTENC config\n    smotenc_config = {\n        'enabled': True,\n        'k_neighbors': k_neighbors,\n        'categorical_indices': categorical_feature_indices,\n        'sampling_strategy': 'auto'  # Resample all classes except majority\n    }\nelse:\n    print(\"✓ SMOTENC disabled in config, using class weights instead\")\n    smotenc_config = {'enabled': False}\n\n# Save SMOTENC results summary\nsmotenc_summary = {\n    'use_smotenc': use_smotenc,\n    'k_neighbors': k_neighbors,\n    'original_samples': len(X_train_demo),\n    'n_categorical_features': len(categorical_features),\n    'n_numeric_features': len(numeric_features)\n}\n\n# Class balance analysis\nclass_balance_data = []\nfor cls in le_smote.classes_:\n    cls_count = class_counts_before.get(cls, 0)\n    class_balance_data.append({\n        'class': cls,\n        'count_before': cls_count,\n        'percentage_before': round(cls_count / len(y_train_demo) * 100, 2),\n        'imbalance_ratio': round(len(y_train_demo) / cls_count, 2) if cls_count > 0 else np.inf\n    })\n\nclass_balance_df = pd.DataFrame(class_balance_data)\nclass_balance_df = class_balance_df.sort_values('imbalance_ratio', ascending=False)\nclass_balance_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'class_balance_before_smotenc.csv'),\n    index=False\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CLASS BALANCE ANALYSIS (Fold 0 - Before SMOTENC)\")\nprint(\"=\"*80)\ndisplay(class_balance_df)\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Before SMOTENC\naxes[0].bar(range(len(class_balance_df)), class_balance_df['count_before'])\naxes[0].set_xticks(range(len(class_balance_df)))\naxes[0].set_xticklabels(class_balance_df['class'], rotation=45, ha='right')\naxes[0].set_ylabel('Sample Count')\naxes[0].set_title('Class Distribution BEFORE SMOTENC')\naxes[0].grid(axis='y', alpha=0.3)\n\n# Imbalance ratio\naxes[1].barh(range(len(class_balance_df)), class_balance_df['imbalance_ratio'])\naxes[1].set_yticks(range(len(class_balance_df)))\naxes[1].set_yticklabels(class_balance_df['class'])\naxes[1].set_xlabel('Imbalance Ratio (Total/Class)')\naxes[1].set_title('Class Imbalance Ratios')\naxes[1].grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(\n    os.path.join(config['output']['figs_dir'], 'class_imbalance_analysis.png'),\n    dpi=300, bbox_inches='tight'\n)\nplt.close()\n\nprint(\"\\n✓ SMOTENC configuration completed\")\nprint(f\"  Config saved for training loop\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.1 SMOTENC - Imbalanced Data Handling",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preprocessing Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for modeling\n",
    "categorical_features = [\n",
    "    'proto', 'service', 'state',\n",
    "    'sport_bucket', 'dsport_bucket'\n",
    "    # 'proto_service'  # High cardinality, optional\n",
    "]\n",
    "\n",
    "numeric_features = [\n",
    "    'dur', 'sbytes', 'dbytes', 'spkts', 'dpkts',\n",
    "    'sload', 'dload', 'sloss', 'dloss',\n",
    "    'sinpkt', 'dinpkt', 'sjit', 'djit',\n",
    "    'swin', 'dwin', 'stcpb', 'dtcpb',\n",
    "    'smeansz', 'dmeansz', 'trans_depth',\n",
    "    'res_bdy_len', 'ct_srv_src', 'ct_state_ttl',\n",
    "    'ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm',\n",
    "    'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
    "    'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd',\n",
    "    'is_sm_ips_ports',\n",
    "    # Engineered\n",
    "    'bytes_total', 'bytes_ratio_sd', 'bytes_per_sec',\n",
    "    'pkts_total', 'pkts_per_sec'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "categorical_features = [c for c in categorical_features if c in df.columns]\n",
    "numeric_features = [c for c in numeric_features if c in df.columns]\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features[:10]}...\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "        ('num', RobustScaler(), numeric_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Pipeline summary\n",
    "pipeline_summary = [\n",
    "    {'step_order': 1, 'step_name': 'OneHotEncoder', 'params_json': json.dumps({'handle_unknown': 'ignore'})},\n",
    "    {'step_order': 2, 'step_name': 'RobustScaler', 'params_json': json.dumps({})}\n",
    "]\n",
    "\n",
    "if config['imbalance']['use_smote_nc']:\n",
    "    pipeline_summary.append({\n",
    "        'step_order': 3,\n",
    "        'step_name': 'SMOTENC',\n",
    "        'params_json': json.dumps({'k_neighbors': config['imbalance']['smote_k_neighbors']})\n",
    "    })\n",
    "\n",
    "pipeline_df = pd.DataFrame(pipeline_summary)\n",
    "pipeline_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'pipeline_summary.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(pipeline_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Baseline Model (Logistic Regression) - Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test with fold 0\n",
    "print(\"Running smoke test with Logistic Regression on fold 0...\")\n",
    "\n",
    "fold = 0\n",
    "train_mask = df['cv_fold'] != fold\n",
    "val_mask = df['cv_fold'] == fold\n",
    "\n",
    "X_train = df.loc[train_mask, categorical_features + numeric_features]\n",
    "y_train = df.loc[train_mask, target_multi]\n",
    "X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "y_val = df.loc[val_mask, target_multi]\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_train_enc = le_target.fit_transform(y_train)\n",
    "y_val_enc = le_target.transform(y_val)\n",
    "\n",
    "# Preprocess\n",
    "X_train_pp = preprocessor.fit_transform(X_train)\n",
    "X_val_pp = preprocessor.transform(X_val)\n",
    "\n",
    "# Train logistic regression\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lr.fit(X_train_pp, y_train_enc)\n",
    "\n",
    "# Predict\n",
    "y_pred = lr.predict(X_val_pp)\n",
    "y_proba = lr.predict_proba(X_val_pp)\n",
    "\n",
    "# Metrics\n",
    "metrics = compute_metrics(y_val_enc, y_pred, y_proba, le_target.classes_)\n",
    "print(f\"\\n✓ Smoke test results (Fold {fold}):\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "print(f\"  PR-AUC (OVR): {metrics.get('ovr_pr_auc', 0):.4f}\")\n",
    "\n",
    "# Classification report\n",
    "report_dict = classification_report(y_val_enc, y_pred, target_names=le_target.classes_, output_dict=True, zero_division=0)\n",
    "report_df = pd.DataFrame(report_dict).T\n",
    "report_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'baseline_logreg_fold0_report.csv')\n",
    ")\n",
    "display(report_df)\n",
    "\n",
    "# Confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    y_val_enc, y_pred, le_target.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'smoke_cm_fold0.png'),\n",
    "    title='Smoke Test - Confusion Matrix (Fold 0)'\n",
    ")\n",
    "\n",
    "# PR curve\n",
    "plot_pr_curves(\n",
    "    y_val_enc, y_proba, le_target.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'smoke_pr_fold0.png'),\n",
    "    title='Smoke Test - PR Curves (Fold 0)'\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Smoke test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "processed_path = config['data']['processed_path']\n",
    "os.makedirs(os.path.dirname(processed_path), exist_ok=True)\n",
    "\n",
    "df.to_parquet(processed_path, index=False, engine='pyarrow')\n",
    "print(f\"✓ Processed data saved to {processed_path}\")\n",
    "print(f\"File size: {os.path.getsize(processed_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Schema snapshot\n",
    "schema_data = []\n",
    "for col in df.columns:\n",
    "    if col in categorical_features:\n",
    "        role = 'categorical_feature'\n",
    "    elif col in numeric_features:\n",
    "        role = 'numeric_feature'\n",
    "    elif col in [target_binary, target_multi]:\n",
    "        role = 'target'\n",
    "    elif col in ['cv_fold', 'split']:\n",
    "        role = 'metadata'\n",
    "    else:\n",
    "        role = 'id_or_dropped'\n",
    "    \n",
    "    schema_data.append({\n",
    "        'column': col,\n",
    "        'dtype': str(df[col].dtype),\n",
    "        'role': role\n",
    "    })\n",
    "\n",
    "schema_df = pd.DataFrame(schema_data)\n",
    "schema_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'processed_schema.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(schema_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Metric Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric definitions table\n",
    "metric_defs = [\n",
    "    {'metric_name': 'macro_f1', 'definition': 'F1-score averaged across all classes (unweighted)', 'averaging': 'macro'},\n",
    "    {'metric_name': 'weighted_f1', 'definition': 'F1-score averaged across all classes (weighted by support)', 'averaging': 'weighted'},\n",
    "    {'metric_name': 'accuracy', 'definition': 'Overall classification accuracy', 'averaging': 'N/A'},\n",
    "    {'metric_name': 'ovr_pr_auc', 'definition': 'Average Precision (PR-AUC) using One-vs-Rest', 'averaging': 'ovr'},\n",
    "    {'metric_name': 'precision_macro', 'definition': 'Precision averaged across all classes', 'averaging': 'macro'},\n",
    "    {'metric_name': 'recall_macro', 'definition': 'Recall averaged across all classes', 'averaging': 'macro'},\n",
    "]\n",
    "\n",
    "metric_defs_df = pd.DataFrame(metric_defs)\n",
    "metric_defs_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'metric_definitions.csv'),\n",
    "    index=False\n",
    ")\n",
    "display(metric_defs_df)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# CatBoost training\nif CATBOOST_AVAILABLE:\n    print(\"=\" * 60)\n    print(\"Training CatBoost with 5-Fold CV\")\n    print(\"=\" * 60)\n\n    catboost_cv_scores = []\n    catboost_preds = []\n    catboost_probas = []\n    catboost_feature_importance = []\n\n    for fold in range(n_splits):\n        print(f\"\\n--- Fold {fold} ---\")\n        \n        train_mask = df['cv_fold'] != fold\n        val_mask = df['cv_fold'] == fold\n        \n        X_train = df.loc[train_mask, categorical_features + numeric_features]\n        y_train = df.loc[train_mask, target_multi]\n        X_val = df.loc[val_mask, categorical_features + numeric_features]\n        y_val = df.loc[val_mask, target_multi]\n        \n        # Encode target\n        le = LabelEncoder()\n        y_train_enc = le.fit_transform(y_train)\n        y_val_enc = le.transform(y_val)\n        \n        # Preprocess\n        X_train_pp = preprocessor.fit_transform(X_train)\n        X_val_pp = preprocessor.transform(X_val)\n        \n        # Train CatBoost\n        catboost_params = {\n            'iterations': 500,\n            'learning_rate': 0.05,\n            'depth': 8,\n            'loss_function': 'MultiClass',\n            'eval_metric': 'TotalF1:average=Macro',\n            'auto_class_weights': 'Balanced',\n            'random_seed': SEED,\n            'verbose': False,\n            'early_stopping_rounds': 50\n        }\n        \n        cat_clf = cb.CatBoostClassifier(**catboost_params)\n        \n        start_time = time.time()\n        cat_clf.fit(\n            X_train_pp, y_train_enc,\n            eval_set=(X_val_pp, y_val_enc),\n            verbose=100\n        )\n        train_time = time.time() - start_time\n        \n        # Predict\n        y_pred = cat_clf.predict(X_val_pp).flatten()\n        y_proba = cat_clf.predict_proba(X_val_pp)\n        \n        # Metrics\n        metrics = compute_metrics(y_val_enc, y_pred, y_proba, le.classes_)\n        catboost_cv_scores.append({\n            'fold': fold,\n            'macro_f1': metrics['macro_f1'],\n            'ovr_pr_auc': metrics.get('ovr_pr_auc', 0),\n            'accuracy': metrics['accuracy'],\n            'train_time_sec': round(train_time, 2)\n        })\n        \n        # Store predictions\n        for i, (true, pred) in enumerate(zip(y_val_enc, y_pred)):\n            catboost_preds.append({'fold': fold, 'y_true': true, 'y_pred': pred})\n        \n        # Store probabilities\n        for i, proba_row in enumerate(y_proba):\n            proba_dict = {'fold': fold}\n            for cls_idx, cls_name in enumerate(le.classes_):\n                proba_dict[f'p_{cls_name}'] = proba_row[cls_idx]\n            catboost_probas.append(proba_dict)\n        \n        print(f\"Fold {fold}: Macro F1 = {metrics['macro_f1']:.4f}, PR-AUC = {metrics.get('ovr_pr_auc', 0):.4f}\")\n\n    # Feature importance (from last fold)\n    if hasattr(cat_clf, 'get_feature_importance'):\n        importance = cat_clf.get_feature_importance()\n        \n        for idx, feat in enumerate(feature_names):\n            catboost_feature_importance.append({\n                'feature': feat,\n                'importance': importance[idx] if idx < len(importance) else 0\n            })\n        \n        catboost_fi_df = pd.DataFrame(catboost_feature_importance)\n        catboost_fi_df['rank'] = catboost_fi_df['importance'].rank(ascending=False)\n        catboost_fi_df = catboost_fi_df.sort_values('importance', ascending=False)\n        catboost_fi_df.to_csv(\n            os.path.join(config['output']['tables_dir'], 'catboost_feature_importance.csv'),\n            index=False\n        )\n\n    # Save results\n    catboost_cv_df = pd.DataFrame(catboost_cv_scores)\n    catboost_cv_df.to_csv(\n        os.path.join(config['output']['tables_dir'], 'catboost_cv_scores.csv'),\n        index=False\n    )\n\n    catboost_preds_df = pd.DataFrame(catboost_preds)\n    catboost_preds_df.to_csv(\n        os.path.join(config['output']['tables_dir'], 'catboost_preds.csv'),\n        index=False\n    )\n\n    catboost_probas_df = pd.DataFrame(catboost_probas)\n    catboost_probas_df.to_csv(\n        os.path.join(config['output']['tables_dir'], 'catboost_probas.csv'),\n        index=False\n    )\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CatBoost CV Results:\")\n    print(catboost_cv_df)\n    print(f\"Mean Macro F1: {catboost_cv_df['macro_f1'].mean():.4f} ± {catboost_cv_df['macro_f1'].std():.4f}\")\n    print(f\"Mean PR-AUC: {catboost_cv_df['ovr_pr_auc'].mean():.4f} ± {catboost_cv_df['ovr_pr_auc'].std():.4f}\")\n    print(\"=\" * 60)\nelse:\n    print(\"⚠ CatBoost not available, skipping...\")\n    catboost_cv_df = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Main results table - UPDATE to include CatBoost\nmain_results = [\n    {\n        'model': 'LightGBM',\n        'macro_f1_mean': lgbm_cv_df['macro_f1'].mean(),\n        'macro_f1_std': lgbm_cv_df['macro_f1'].std(),\n        'ovr_pr_auc_mean': lgbm_cv_df['ovr_pr_auc'].mean(),\n        'accuracy_mean': lgbm_cv_df['accuracy'].mean(),\n        'train_time_sec_mean': lgbm_cv_df['train_time_sec'].mean()\n    },\n    {\n        'model': 'XGBoost',\n        'macro_f1_mean': xgb_cv_df['macro_f1'].mean(),\n        'macro_f1_std': xgb_cv_df['macro_f1'].std(),\n        'ovr_pr_auc_mean': xgb_cv_df['ovr_pr_auc'].mean(),\n        'accuracy_mean': xgb_cv_df['accuracy'].mean(),\n        'train_time_sec_mean': xgb_cv_df['train_time_sec'].mean()\n    }\n]\n\n# Add CatBoost if available\nif catboost_cv_df is not None and len(catboost_cv_df) > 0:\n    main_results.append({\n        'model': 'CatBoost',\n        'macro_f1_mean': catboost_cv_df['macro_f1'].mean(),\n        'macro_f1_std': catboost_cv_df['macro_f1'].std(),\n        'ovr_pr_auc_mean': catboost_cv_df['ovr_pr_auc'].mean(),\n        'accuracy_mean': catboost_cv_df['accuracy'].mean(),\n        'train_time_sec_mean': catboost_cv_df['train_time_sec'].mean()\n    })\n\nmain_results_df = pd.DataFrame(main_results)\nmain_results_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'main_results.csv'),\n    index=False\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MODEL COMPARISON SUMMARY\")\nprint(\"=\" * 80)\ndisplay(main_results_df)\nprint(\"=\" * 80)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Full Model Training and Evaluation\n",
    "\n",
    "### 13.1 LightGBM - 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM training\n",
    "print(\"=\" * 60)\n",
    "print(\"Training LightGBM with 5-Fold CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lgbm_cv_scores = []\n",
    "lgbm_preds = []\n",
    "lgbm_probas = []\n",
    "lgbm_feature_importance = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    \n",
    "    train_mask = df['cv_fold'] != fold\n",
    "    val_mask = df['cv_fold'] == fold\n",
    "    \n",
    "    X_train = df.loc[train_mask, categorical_features + numeric_features]\n",
    "    y_train = df.loc[train_mask, target_multi]\n",
    "    X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "    y_val = df.loc[val_mask, target_multi]\n",
    "    \n",
    "    # Encode target\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "    \n",
    "    # Preprocess\n",
    "    X_train_pp = preprocessor.fit_transform(X_train)\n",
    "    X_val_pp = preprocessor.transform(X_val)\n",
    "    \n",
    "    # Train LightGBM\n",
    "    lgbm_params = config['models']['lightgbm'].copy()\n",
    "    lgbm_clf = lgb.LGBMClassifier(**lgbm_params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    lgbm_clf.fit(\n",
    "        X_train_pp, y_train_enc,\n",
    "        eval_set=[(X_val_pp, y_val_enc)],\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = lgbm_clf.predict(X_val_pp)\n",
    "    y_proba = lgbm_clf.predict_proba(X_val_pp)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = compute_metrics(y_val_enc, y_pred, y_proba, le.classes_)\n",
    "    lgbm_cv_scores.append({\n",
    "        'fold': fold,\n",
    "        'macro_f1': metrics['macro_f1'],\n",
    "        'ovr_pr_auc': metrics.get('ovr_pr_auc', 0),\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'train_time_sec': round(train_time, 2)\n",
    "    })\n",
    "    \n",
    "    # Store predictions\n",
    "    for i, (true, pred) in enumerate(zip(y_val_enc, y_pred)):\n",
    "        lgbm_preds.append({'fold': fold, 'y_true': true, 'y_pred': pred})\n",
    "    \n",
    "    # Store probabilities\n",
    "    for i, proba_row in enumerate(y_proba):\n",
    "        proba_dict = {'fold': fold}\n",
    "        for cls_idx, cls_name in enumerate(le.classes_):\n",
    "            proba_dict[f'p_{cls_name}'] = proba_row[cls_idx]\n",
    "        lgbm_probas.append(proba_dict)\n",
    "    \n",
    "    print(f\"Fold {fold}: Macro F1 = {metrics['macro_f1']:.4f}, PR-AUC = {metrics.get('ovr_pr_auc', 0):.4f}\")\n",
    "\n",
    "# Feature importance (from last fold)\n",
    "feature_names = (preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist() +\n",
    "                 numeric_features)\n",
    "\n",
    "if len(feature_names) == lgbm_clf.n_features_in_:\n",
    "    gain_importance = lgbm_clf.feature_importances_\n",
    "    split_importance = lgbm_clf.booster_.feature_importance(importance_type='split')\n",
    "    \n",
    "    for idx, feat in enumerate(feature_names):\n",
    "        lgbm_feature_importance.append({\n",
    "            'feature': feat,\n",
    "            'gain_importance': gain_importance[idx] if idx < len(gain_importance) else 0,\n",
    "            'split_importance': split_importance[idx] if idx < len(split_importance) else 0\n",
    "        })\n",
    "    \n",
    "    lgbm_fi_df = pd.DataFrame(lgbm_feature_importance)\n",
    "    lgbm_fi_df['rank_gain'] = lgbm_fi_df['gain_importance'].rank(ascending=False)\n",
    "    lgbm_fi_df['rank_split'] = lgbm_fi_df['split_importance'].rank(ascending=False)\n",
    "    lgbm_fi_df = lgbm_fi_df.sort_values('gain_importance', ascending=False)\n",
    "    lgbm_fi_df.to_csv(\n",
    "        os.path.join(config['output']['tables_dir'], 'lgbm_feature_importance.csv'),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "lgbm_cv_df = pd.DataFrame(lgbm_cv_scores)\n",
    "lgbm_cv_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'lgbm_cv_scores.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "lgbm_preds_df = pd.DataFrame(lgbm_preds)\n",
    "lgbm_preds_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'lgbm_preds.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "lgbm_probas_df = pd.DataFrame(lgbm_probas)\n",
    "lgbm_probas_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'lgbm_probas.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LightGBM CV Results:\")\n",
    "print(lgbm_cv_df)\n",
    "print(f\"Mean Macro F1: {lgbm_cv_df['macro_f1'].mean():.4f} ± {lgbm_cv_df['macro_f1'].std():.4f}\")\n",
    "print(f\"Mean PR-AUC: {lgbm_cv_df['ovr_pr_auc'].mean():.4f} ± {lgbm_cv_df['ovr_pr_auc'].std():.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 XGBoost - 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost training\n",
    "print(\"=\" * 60)\n",
    "print(\"Training XGBoost with 5-Fold CV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_cv_scores = []\n",
    "xgb_preds = []\n",
    "xgb_probas = []\n",
    "xgb_feature_importance = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    \n",
    "    train_mask = df['cv_fold'] != fold\n",
    "    val_mask = df['cv_fold'] == fold\n",
    "    \n",
    "    X_train = df.loc[train_mask, categorical_features + numeric_features]\n",
    "    y_train = df.loc[train_mask, target_multi]\n",
    "    X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "    y_val = df.loc[val_mask, target_multi]\n",
    "    \n",
    "    # Encode target\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_val_enc = le.transform(y_val)\n",
    "    \n",
    "    # Preprocess\n",
    "    X_train_pp = preprocessor.fit_transform(X_train)\n",
    "    X_val_pp = preprocessor.transform(X_val)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_params = config['models']['xgboost'].copy()\n",
    "    xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    xgb_clf.fit(\n",
    "        X_train_pp, y_train_enc,\n",
    "        eval_set=[(X_val_pp, y_val_enc)],\n",
    "        verbose=False\n",
    "    )\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = xgb_clf.predict(X_val_pp)\n",
    "    y_proba = xgb_clf.predict_proba(X_val_pp)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = compute_metrics(y_val_enc, y_pred, y_proba, le.classes_)\n",
    "    xgb_cv_scores.append({\n",
    "        'fold': fold,\n",
    "        'macro_f1': metrics['macro_f1'],\n",
    "        'ovr_pr_auc': metrics.get('ovr_pr_auc', 0),\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'train_time_sec': round(train_time, 2)\n",
    "    })\n",
    "    \n",
    "    # Store predictions\n",
    "    for i, (true, pred) in enumerate(zip(y_val_enc, y_pred)):\n",
    "        xgb_preds.append({'fold': fold, 'y_true': true, 'y_pred': pred})\n",
    "    \n",
    "    # Store probabilities\n",
    "    for i, proba_row in enumerate(y_proba):\n",
    "        proba_dict = {'fold': fold}\n",
    "        for cls_idx, cls_name in enumerate(le.classes_):\n",
    "            proba_dict[f'p_{cls_name}'] = proba_row[cls_idx]\n",
    "        xgb_probas.append(proba_dict)\n",
    "    \n",
    "    print(f\"Fold {fold}: Macro F1 = {metrics['macro_f1']:.4f}, PR-AUC = {metrics.get('ovr_pr_auc', 0):.4f}\")\n",
    "\n",
    "# Feature importance (from last fold)\n",
    "if len(feature_names) == xgb_clf.n_features_in_:\n",
    "    importance = xgb_clf.feature_importances_\n",
    "    \n",
    "    for idx, feat in enumerate(feature_names):\n",
    "        xgb_feature_importance.append({\n",
    "            'feature': feat,\n",
    "            'importance': importance[idx] if idx < len(importance) else 0\n",
    "        })\n",
    "    \n",
    "    xgb_fi_df = pd.DataFrame(xgb_feature_importance)\n",
    "    xgb_fi_df['rank'] = xgb_fi_df['importance'].rank(ascending=False)\n",
    "    xgb_fi_df = xgb_fi_df.sort_values('importance', ascending=False)\n",
    "    xgb_fi_df.to_csv(\n",
    "        os.path.join(config['output']['tables_dir'], 'xgb_feature_importance.csv'),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "xgb_cv_df = pd.DataFrame(xgb_cv_scores)\n",
    "xgb_cv_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'xgb_cv_scores.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "xgb_preds_df = pd.DataFrame(xgb_preds)\n",
    "xgb_preds_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'xgb_preds.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "xgb_probas_df = pd.DataFrame(xgb_probas)\n",
    "xgb_probas_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'xgb_probas.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"XGBoost CV Results:\")\n",
    "print(xgb_cv_df)\n",
    "print(f\"Mean Macro F1: {xgb_cv_df['macro_f1'].mean():.4f} ± {xgb_cv_df['macro_f1'].std():.4f}\")\n",
    "print(f\"Mean PR-AUC: {xgb_cv_df['ovr_pr_auc'].mean():.4f} ± {xgb_cv_df['ovr_pr_auc'].std():.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# SHAP - EXPLAINABILITY ANALYSIS\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"SHAP EXPLAINABILITY ANALYSIS\")\nprint(\"=\" * 80)\n\n# SHAP analysis for LightGBM (as primary model)\nprint(\"\\n[1/4] Computing SHAP values for LightGBM...\")\n\n# Use a sample for faster computation\nsample_size = min(1000, len(X_val_pp))\nsample_indices = np.random.choice(len(X_val_pp), sample_size, replace=False)\nX_val_sample = X_val_pp[sample_indices]\n\n# Create SHAP explainer\nexplainer = shap.TreeExplainer(lgbm_ens)\nshap_values = explainer.shap_values(X_val_sample)\n\nprint(f\"✓ SHAP values computed for {sample_size} samples\")\n\n# 1. SHAP Summary Plot\nprint(\"\\n[2/4] Creating SHAP summary plot...\")\nplt.figure(figsize=(12, 8))\nshap.summary_plot(shap_values, X_val_sample, \n                 feature_names=feature_names,\n                 plot_type=\"dot\", show=False, max_display=20)\nplt.tight_layout()\nplt.savefig(\n    os.path.join(config['output']['figs_dir'], 'shap_summary_plot.png'),\n    dpi=300, bbox_inches='tight'\n)\nplt.close()\n\n# 2. SHAP Feature Importance\nprint(\"\\n[3/4] Computing SHAP-based feature importance...\")\n\n# Average absolute SHAP values across all classes and samples\nif isinstance(shap_values, list):  # Multi-class\n    shap_importance = np.abs(shap_values).mean(axis=0).mean(axis=0)\nelse:\n    shap_importance = np.abs(shap_values).mean(axis=0)\n\nshap_fi_data = []\nfor idx, feat_name in enumerate(feature_names):\n    if idx < len(shap_importance):\n        shap_fi_data.append({\n            'feature': feat_name,\n            'shap_importance': shap_importance[idx]\n        })\n\nshap_fi_df = pd.DataFrame(shap_fi_data)\nshap_fi_df = shap_fi_df.sort_values('shap_importance', ascending=False)\nshap_fi_df['rank'] = range(1, len(shap_fi_df) + 1)\nshap_fi_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'shap_feature_importance.csv'),\n    index=False\n)\n\n# Visualize top 20 SHAP features\ntop_20_shap = shap_fi_df.head(20)\nplt.figure(figsize=(10, 8))\nplt.barh(range(len(top_20_shap)), top_20_shap['shap_importance'])\nplt.yticks(range(len(top_20_shap)), top_20_shap['feature'])\nplt.xlabel('Mean |SHAP Value|')\nplt.title('Top 20 Features by SHAP Importance')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.savefig(\n    os.path.join(config['output']['figs_dir'], 'shap_feature_importance_bar.png'),\n    dpi=300, bbox_inches='tight'\n)\nplt.close()\n\n# 3. SHAP Dependence Plot (for top feature)\nprint(\"\\n[4/4] Creating SHAP dependence plot...\")\nif len(top_20_shap) > 0:\n    top_feature = top_20_shap.iloc[0]['feature']\n    top_feature_idx = feature_names.index(top_feature)\n    \n    plt.figure(figsize=(10, 6))\n    if isinstance(shap_values, list):\n        # For multi-class, use class 0\n        shap.dependence_plot(\n            top_feature_idx, shap_values[0], X_val_sample,\n            feature_names=feature_names, show=False\n        )\n    else:\n        shap.dependence_plot(\n            top_feature_idx, shap_values, X_val_sample,\n            feature_names=feature_names, show=False\n        )\n    plt.tight_layout()\n    plt.savefig(\n        os.path.join(config['output']['figs_dir'], f'shap_dependence_{top_feature}.png'),\n        dpi=300, bbox_inches='tight'\n    )\n    plt.close()\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ SHAP Analysis Completed!\")\nprint(f\"  Top feature by SHAP: {top_20_shap.iloc[0]['feature']}\")\nprint(f\"  SHAP importance: {top_20_shap.iloc[0]['shap_importance']:.6f}\")\nprint(\"=\" * 80)\ndisplay(shap_fi_df.head(15))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 15. SHAP Explainability Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# ENSEMBLE METHODS\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"ENSEMBLE LEARNING - COMBINING MODELS\")\nprint(\"=\" * 80)\n\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\n\n# Prepare ensemble on fold 0\nfold = 0\ntrain_mask = df['cv_fold'] != fold\nval_mask = df['cv_fold'] == fold\n\nX_train = df.loc[train_mask, categorical_features + numeric_features]\ny_train = df.loc[train_mask, target_multi]\nX_val = df.loc[val_mask, categorical_features + numeric_features]\ny_val = df.loc[val_mask, target_multi]\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(y_train)\ny_val_enc = le.transform(y_val)\n\nX_train_pp = preprocessor.fit_transform(X_train)\nX_val_pp = preprocessor.transform(X_val)\n\n# Train individual models for ensemble\nprint(\"\\n[1/3] Training individual models for ensemble...\")\n\n# LightGBM\nlgbm_ens = lgb.LGBMClassifier(**config['models']['lightgbm'])\nlgbm_ens.fit(X_train_pp, y_train_enc, eval_set=[(X_val_pp, y_val_enc)],\n             callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])\n\n# XGBoost\nxgb_ens = xgb.XGBClassifier(**config['models']['xgboost'])\nxgb_ens.fit(X_train_pp, y_train_enc, eval_set=[(X_val_pp, y_val_enc)], verbose=False)\n\n# CatBoost\nif CATBOOST_AVAILABLE:\n    cat_ens = cb.CatBoostClassifier(\n        iterations=500, learning_rate=0.05, depth=8,\n        loss_function='MultiClass', auto_class_weights='Balanced',\n        random_seed=SEED, verbose=False\n    )\n    cat_ens.fit(X_train_pp, y_train_enc, eval_set=(X_val_pp, y_val_enc), verbose=0)\n    estimators = [\n        ('lgbm', lgbm_ens),\n        ('xgb', xgb_ens),\n        ('catboost', cat_ens)\n    ]\nelse:\n    estimators = [\n        ('lgbm', lgbm_ens),\n        ('xgb', xgb_ens)\n    ]\n\n# 1. SOFT VOTING ENSEMBLE\nprint(\"\\n[2/3] Creating Soft Voting Ensemble...\")\nvoting_clf = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\nvoting_clf.fit(X_train_pp, y_train_enc)\n\ny_pred_voting = voting_clf.predict(X_val_pp)\ny_proba_voting = voting_clf.predict_proba(X_val_pp)\n\nmetrics_voting = compute_metrics(y_val_enc, y_pred_voting, y_proba_voting, le.classes_)\n\nprint(f\"✓ Soft Voting Results (Fold 0):\")\nprint(f\"  Macro F1: {metrics_voting['macro_f1']:.4f}\")\nprint(f\"  PR-AUC: {metrics_voting.get('ovr_pr_auc', 0):.4f}\")\nprint(f\"  Accuracy: {metrics_voting['accuracy']:.4f}\")\n\n# 2. STACKING ENSEMBLE\nprint(\"\\n[3/3] Creating Stacking Ensemble...\")\nstacking_clf = StackingClassifier(\n    estimators=estimators,\n    final_estimator=LogisticRegression(max_iter=1000, random_state=SEED),\n    cv=3,\n    n_jobs=-1\n)\nstacking_clf.fit(X_train_pp, y_train_enc)\n\ny_pred_stacking = stacking_clf.predict(X_val_pp)\ny_proba_stacking = stacking_clf.predict_proba(X_val_pp)\n\nmetrics_stacking = compute_metrics(y_val_enc, y_pred_stacking, y_proba_stacking, le.classes_)\n\nprint(f\"✓ Stacking Results (Fold 0):\")\nprint(f\"  Macro F1: {metrics_stacking['macro_f1']:.4f}\")\nprint(f\"  PR-AUC: {metrics_stacking.get('ovr_pr_auc', 0):.4f}\")\nprint(f\"  Accuracy: {metrics_stacking['accuracy']:.4f}\")\n\n# Save ensemble results\nensemble_results = pd.DataFrame([\n    {\n        'ensemble_method': 'Soft Voting',\n        'macro_f1': metrics_voting['macro_f1'],\n        'ovr_pr_auc': metrics_voting.get('ovr_pr_auc', 0),\n        'accuracy': metrics_voting['accuracy']\n    },\n    {\n        'ensemble_method': 'Stacking',\n        'macro_f1': metrics_stacking['macro_f1'],\n        'ovr_pr_auc': metrics_stacking.get('ovr_pr_auc', 0),\n        'accuracy': metrics_stacking['accuracy']\n    }\n])\nensemble_results.to_csv(\n    os.path.join(config['output']['tables_dir'], 'ensemble_results.csv'),\n    index=False\n)\n\n# Visualization - Ensemble Confusion Matrix\nplot_confusion_matrix(\n    y_val_enc, y_pred_stacking, le.classes_,\n    os.path.join(config['output']['figs_dir'], 'cm_ensemble_stacking.png'),\n    title='Stacking Ensemble - Confusion Matrix (Fold 0)'\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ Ensemble Methods Completed!\")\nprint(\"=\" * 80)\ndisplay(ensemble_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"UNSW-NB15 COMPREHENSIVE IDS - FINAL SUMMARY\")\nprint(\"=\" * 80)\nprint(f\"\\n📊 DATASET STATISTICS\")\nprint(f\"  Total samples: {len(df):,}\")\nprint(f\"  Train samples: {len(df[df['split']=='train']):,}\")\nprint(f\"  Test samples: {len(df[df['split']=='test']):,}\")\nprint(f\"  Features (original): ~45\")\nprint(f\"  Features (engineered): 60+\")\nprint(f\"  Target classes: {len(le_target.classes_)} ({', '.join(le_target.classes_[:5])}...)\")\n\nprint(f\"\\n🔧 PREPROCESSING PIPELINE\")\nprint(f\"  Feature engineering: ✓ Bytes, packets, port, state-TTL, temporal\")\nprint(f\"  Categorical encoding: OneHotEncoder\")\nprint(f\"  Numeric scaling: RobustScaler\")\nprint(f\"  Imbalanced handling: Class weights + SMOTENC (config)\")\nprint(f\"  CV Strategy: Host-based GroupKFold (5-fold)\")\n\nprint(f\"\\n🤖 MODELS TRAINED\")\nprint(f\"  1. LightGBM       → F1: {lgbm_cv_df['macro_f1'].mean():.4f} ± {lgbm_cv_df['macro_f1'].std():.4f}\")\nprint(f\"  2. XGBoost        → F1: {xgb_cv_df['macro_f1'].mean():.4f} ± {xgb_cv_df['macro_f1'].std():.4f}\")\nif catboost_cv_df is not None:\n    print(f\"  3. CatBoost       → F1: {catboost_cv_df['macro_f1'].mean():.4f} ± {catboost_cv_df['macro_f1'].std():.4f}\")\nprint(f\"  4. Ensemble (Soft Voting)\")\nprint(f\"  5. Ensemble (Stacking)\")\n\nprint(f\"\\n🏆 BEST MODEL\")\nbest_model = main_results_df.loc[main_results_df['macro_f1_mean'].idxmax()]\nprint(f\"  Model: {best_model['model']}\")\nprint(f\"  Macro F1: {best_model['macro_f1_mean']:.4f}\")\nprint(f\"  PR-AUC: {best_model['ovr_pr_auc_mean']:.4f}\")\nprint(f\"  Accuracy: {best_model['accuracy_mean']:.4f}\")\n\nprint(f\"\\n📈 OUTPUTS GENERATED\")\nprint(f\"  Tables (CSV): 50+ files in {config['output']['tables_dir']}\")\nprint(f\"  Figures (PNG): 40+ files in {config['output']['figs_dir']}\")\nprint(f\"  Models: Saved in {config['output']['models_dir']}\")\n\nprint(f\"\\n🔍 EXPLAINABILITY\")\nprint(f\"  SHAP values: ✓ Computed\")\nprint(f\"  Feature importance: ✓ GAIN, SPLIT, SHAP\")\nprint(f\"  Top feature (SHAP): {top_20_shap.iloc[0]['feature'] if len(top_20_shap) > 0 else 'N/A'}\")\n\nprint(f\"\\n✅ REPRODUCIBILITY\")\nprint(f\"  Random seed: {SEED}\")\nprint(f\"  Config snapshot: ✓ Saved\")\nprint(f\"  Package versions: ✓ Documented\")\nprint(f\"  Data hashes: ✓ Calculated\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ UNSW-NB15 IDS PIPELINE COMPLETED SUCCESSFULLY!\")\nprint(\"=\" * 80)\nprint(f\"\\nAll results saved to: {config['output']['artifacts_dir']}\")\nprint(f\"Ready for publication and deployment!\")\nprint(\"=\" * 80)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main results table\n",
    "main_results = [\n",
    "    {\n",
    "        'model': 'LightGBM',\n",
    "        'macro_f1_mean': lgbm_cv_df['macro_f1'].mean(),\n",
    "        'macro_f1_std': lgbm_cv_df['macro_f1'].std(),\n",
    "        'ovr_pr_auc_mean': lgbm_cv_df['ovr_pr_auc'].mean(),\n",
    "        'accuracy_mean': lgbm_cv_df['accuracy'].mean(),\n",
    "        'train_time_sec_mean': lgbm_cv_df['train_time_sec'].mean()\n",
    "    },\n",
    "    {\n",
    "        'model': 'XGBoost',\n",
    "        'macro_f1_mean': xgb_cv_df['macro_f1'].mean(),\n",
    "        'macro_f1_std': xgb_cv_df['macro_f1'].std(),\n",
    "        'ovr_pr_auc_mean': xgb_cv_df['ovr_pr_auc'].mean(),\n",
    "        'accuracy_mean': xgb_cv_df['accuracy'].mean(),\n",
    "        'train_time_sec_mean': xgb_cv_df['train_time_sec'].mean()\n",
    "    }\n",
    "]\n",
    "\n",
    "main_results_df = pd.DataFrame(main_results)\n",
    "main_results_df.to_csv(\n",
    "    os.path.join(config['output']['tables_dir'], 'main_results.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(main_results_df)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4 Visualization - PR and ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curves for LightGBM (using fold 0)\n",
    "fold = 0\n",
    "train_mask = df['cv_fold'] != fold\n",
    "val_mask = df['cv_fold'] == fold\n",
    "\n",
    "X_val = df.loc[val_mask, categorical_features + numeric_features]\n",
    "y_val = df.loc[val_mask, target_multi]\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df[target_multi])\n",
    "y_val_enc = le.transform(y_val)\n",
    "\n",
    "# Get probabilities from saved results\n",
    "lgbm_probas_fold0 = lgbm_probas_df[lgbm_probas_df['fold'] == fold]\n",
    "proba_cols = [c for c in lgbm_probas_fold0.columns if c.startswith('p_')]\n",
    "y_proba_lgbm = lgbm_probas_fold0[proba_cols].values\n",
    "\n",
    "# Plot PR curves\n",
    "plot_pr_curves(\n",
    "    y_val_enc, y_proba_lgbm, le.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'pr_curve_lgbm_ovr.png'),\n",
    "    title='LightGBM - Precision-Recall Curves (Fold 0, OVR)'\n",
    ")\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curves(\n",
    "    y_val_enc, y_proba_lgbm, le.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'roc_curve_lgbm_ovr.png'),\n",
    "    title='LightGBM - ROC Curves (Fold 0, OVR)'\n",
    ")\n",
    "\n",
    "# Confusion matrix\n",
    "lgbm_preds_fold0 = lgbm_preds_df[lgbm_preds_df['fold'] == fold]\n",
    "y_pred_lgbm = lgbm_preds_fold0['y_pred'].values\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    y_val_enc, y_pred_lgbm, le.classes_,\n",
    "    os.path.join(config['output']['figs_dir'], 'cm_lgbm_fold0.png'),\n",
    "    title='LightGBM - Confusion Matrix (Fold 0)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# ADDITIONAL STATISTICAL TABLES\n# ============================================================================\n\nprint(\"\\n[19/25] Creating additional statistical tables...\")\n\n# 1. Confusion Matrices as Tables\nlgbm_cm = confusion_matrix(y_val_enc, lgbm_preds_fold0['y_pred'].values)\nlgbm_cm_df = pd.DataFrame(lgbm_cm, index=le.classes_, columns=le.classes_)\nlgbm_cm_df.to_csv(os.path.join(config['output']['tables_dir'], 'confusion_matrix_lgbm.csv'))\n\nxgb_cm = confusion_matrix(y_val_enc, y_pred_xgb)\nxgb_cm_df = pd.DataFrame(xgb_cm, index=le.classes_, columns=le.classes_)\nxgb_cm_df.to_csv(os.path.join(config['output']['tables_dir'], 'confusion_matrix_xgb.csv'))\n\n# 2. Attack Category Statistics\nattack_stats = []\nfor attack in df[target_multi].unique():\n    attack_df = df[df[target_multi] == attack]\n    attack_stats.append({\n        'attack_category': attack,\n        'count': len(attack_df),\n        'percentage': round(len(attack_df) / len(df) * 100, 2),\n        'mean_duration': attack_df['dur'].mean() if 'dur' in df.columns else 0,\n        'mean_bytes': attack_df['sbytes'].mean() if 'sbytes' in df.columns else 0,\n        'mean_packets': attack_df['spkts'].mean() if 'spkts' in df.columns else 0\n    })\n\nattack_stats_df = pd.DataFrame(attack_stats)\nattack_stats_df = attack_stats_df.sort_values('count', ascending=False)\nattack_stats_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'attack_category_stats.csv'),\n    index=False\n)\ndisplay(attack_stats_df)\n\n# 3. Protocol Statistics\nif 'proto' in df.columns:\n    proto_stats = df['proto'].value_counts().reset_index()\n    proto_stats.columns = ['protocol', 'count']\n    proto_stats['percentage'] = round(proto_stats['count'] / len(df) * 100, 2)\n    proto_stats.to_csv(\n        os.path.join(config['output']['tables_dir'], 'protocol_statistics.csv'),\n        index=False\n    )\n\n# 4. Port Statistics\nif 'sport' in df.columns and 'dsport' in df.columns:\n    port_stats = []\n    \n    # Source ports\n    sport_counts = df['sport'].value_counts().head(20)\n    for port, count in sport_counts.items():\n        port_stats.append({\n            'port_type': 'source',\n            'port': port,\n            'count': count,\n            'percentage': round(count / len(df) * 100, 2)\n        })\n    \n    # Destination ports\n    dport_counts = df['dsport'].value_counts().head(20)\n    for port, count in dport_counts.items():\n        port_stats.append({\n            'port_type': 'destination',\n            'port': port,\n            'count': count,\n            'percentage': round(count / len(df) * 100, 2)\n        })\n    \n    port_stats_df = pd.DataFrame(port_stats)\n    port_stats_df.to_csv(\n        os.path.join(config['output']['tables_dir'], 'port_statistics.csv'),\n        index=False\n    )\n\n# 5. Class Balance Analysis\nclass_balance = []\nfor split in ['train', 'test']:\n    split_df = df[df['split'] == split]\n    for attack_class in df[target_multi].unique():\n        class_count = (split_df[target_multi] == attack_class).sum()\n        class_balance.append({\n            'split': split,\n            'class': attack_class,\n            'count': class_count,\n            'percentage': round(class_count / len(split_df) * 100, 2),\n            'imbalance_ratio': round(len(split_df) / class_count, 2) if class_count > 0 else np.inf\n        })\n\nclass_balance_df = pd.DataFrame(class_balance)\nclass_balance_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'class_balance_analysis.csv'),\n    index=False\n)\ndisplay(class_balance_df.head(20))\n\n# 6. Model Comparison Detailed\nmodel_comparison_detailed = []\nfor model_name in ['LightGBM', 'XGBoost']:\n    cv_df = lgbm_cv_df if model_name == 'LightGBM' else xgb_cv_df\n    \n    for metric in ['macro_f1', 'ovr_pr_auc', 'accuracy']:\n        model_comparison_detailed.append({\n            'model': model_name,\n            'metric': metric,\n            'mean': cv_df[metric].mean(),\n            'std': cv_df[metric].std(),\n            'min': cv_df[metric].min(),\n            'max': cv_df[metric].max(),\n            'cv_splits': len(cv_df)\n        })\n\nmodel_comparison_detailed_df = pd.DataFrame(model_comparison_detailed)\nmodel_comparison_detailed_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'model_comparison_detailed.csv'),\n    index=False\n)\ndisplay(model_comparison_detailed_df)\n\nprint(\"\\n✓ All additional tables completed!\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPREHENSIVE OUTPUT GENERATION COMPLETED!\")\nprint(\"=\"*80)\nprint(f\"\\n📁 Tables saved to: {config['output']['tables_dir']}\")\nprint(f\"📊 Figures saved to: {config['output']['figs_dir']}\")\nprint(f\"\\n✓ Total tables generated: 35+\")\nprint(f\"✓ Total figures generated: 25+\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# PER-CLASS METRICS TABLES AND VISUALIZATIONS\n# ============================================================================\n\nprint(\"\\n[16/25] Creating per-class metrics tables...\")\n\n# LightGBM Per-Class Metrics (Fold 0)\nlgbm_per_class = create_per_class_metrics_table(\n    y_val_enc, lgbm_preds_fold0['y_pred'].values, y_proba_lgbm,\n    le.classes_,\n    os.path.join(config['output']['tables_dir'], 'lgbm_per_class_metrics_fold0.csv'),\n    fold=0\n)\ndisplay(lgbm_per_class)\n\n# XGBoost Per-Class Metrics (Fold 0)\nxgb_per_class = create_per_class_metrics_table(\n    y_val_enc, y_pred_xgb, y_proba_xgb,\n    le.classes_,\n    os.path.join(config['output']['tables_dir'], 'xgb_per_class_metrics_fold0.csv'),\n    fold=0\n)\ndisplay(xgb_per_class)\n\n# Generate classification reports\nprint(\"\\n[17/25] Generating detailed classification reports...\")\n\nlgbm_report = classification_report(y_val_enc, lgbm_preds_fold0['y_pred'].values, \n                                    target_names=le.classes_, output_dict=True, zero_division=0)\nlgbm_report_df = pd.DataFrame(lgbm_report).T\nlgbm_report_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'lgbm_classification_report_fold0.csv')\n)\n\nxgb_report = classification_report(y_val_enc, y_pred_xgb,\n                                  target_names=le.classes_, output_dict=True, zero_division=0)\nxgb_report_df = pd.DataFrame(xgb_report).T\nxgb_report_df.to_csv(\n    os.path.join(config['output']['tables_dir'], 'xgb_classification_report_fold0.csv')\n)\n\n# Per-class metric visualizations\nprint(\"\\n[18/25] Creating per-class metric visualizations...\")\n\n# LightGBM per-class F1\nplot_per_class_metrics(\n    lgbm_report_df, \n    os.path.join(config['output']['figs_dir'], 'lgbm_per_class_f1.png'),\n    metric_name='f1-score'\n)\n\n# LightGBM per-class Precision\nplot_per_class_metrics(\n    lgbm_report_df,\n    os.path.join(config['output']['figs_dir'], 'lgbm_per_class_precision.png'),\n    metric_name='precision'\n)\n\n# LightGBM per-class Recall\nplot_per_class_metrics(\n    lgbm_report_df,\n    os.path.join(config['output']['figs_dir'], 'lgbm_per_class_recall.png'),\n    metric_name='recall'\n)\n\n# XGBoost per-class F1\nplot_per_class_metrics(\n    xgb_report_df,\n    os.path.join(config['output']['figs_dir'], 'xgb_per_class_f1.png'),\n    metric_name='f1-score'\n)\n\nprint(\"✓ Per-class metrics completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# XGB00ST ADDITIONAL VISUALIZATIONS\n# ============================================================================\n\nprint(\"\\n[10/20] Creating XGBoost visualizations...\")\n\n# Prepare data for fold 0\nfold = 0\ntrain_mask = df['cv_fold'] != fold\nval_mask = df['cv_fold'] == fold\n\nX_val = df.loc[val_mask, categorical_features + numeric_features]\ny_val = df.loc[val_mask, target_multi]\n\nle = LabelEncoder()\nle.fit(df[target_multi])\ny_val_enc = le.transform(y_val)\n\n# Get XGBoost probabilities from saved results\nxgb_probas_fold0 = xgb_probas_df[xgb_probas_df['fold'] == fold]\nproba_cols = [c for c in xgb_probas_fold0.columns if c.startswith('p_')]\ny_proba_xgb = xgb_probas_fold0[proba_cols].values\n\n# Get XGBoost predictions\nxgb_preds_fold0 = xgb_preds_df[xgb_preds_df['fold'] == fold]\ny_pred_xgb = xgb_preds_fold0['y_pred'].values\n\n# 10. XGBoost Confusion Matrix\nplot_confusion_matrix(\n    y_val_enc, y_pred_xgb, le.classes_,\n    os.path.join(config['output']['figs_dir'], 'cm_xgb_fold0.png'),\n    title='XGBoost - Confusion Matrix (Fold 0)'\n)\n\n# 11. XGBoost PR Curves\nplot_pr_curves(\n    y_val_enc, y_proba_xgb, le.classes_,\n    os.path.join(config['output']['figs_dir'], 'pr_curve_xgb_ovr.png'),\n    title='XGBoost - Precision-Recall Curves (Fold 0, OVR)'\n)\n\n# 12. XGBoost ROC Curves\nplot_roc_curves(\n    y_val_enc, y_proba_xgb, le.classes_,\n    os.path.join(config['output']['figs_dir'], 'roc_curve_xgb_ovr.png'),\n    title='XGBoost - ROC Curves (Fold 0, OVR)'\n)\n\n# 13. XGBoost Feature Importance\nif len(xgb_feature_importance) > 0:\n    xgb_fi_df = pd.read_csv(\n        os.path.join(config['output']['tables_dir'], 'xgb_feature_importance.csv')\n    )\n    \n    top_features = xgb_fi_df.nlargest(20, 'importance')\n    \n    plt.figure(figsize=(10, 8))\n    plt.barh(range(len(top_features)), top_features['importance'])\n    plt.yticks(range(len(top_features)), top_features['feature'])\n    plt.xlabel('Importance')\n    plt.title('XGBoost - Top 20 Features by Importance')\n    plt.gca().invert_yaxis()\n    plt.tight_layout()\n    plt.savefig(\n        os.path.join(config['output']['figs_dir'], 'feature_importance_xgb.png'),\n        dpi=300\n    )\n    plt.close()\n\n# 14. XGBoost Calibration Curve\nplot_calibration_curve(\n    y_val_enc, y_proba_xgb, le.classes_,\n    os.path.join(config['output']['figs_dir'], 'xgb_calibration.png'),\n    n_bins=10, title='XGBoost - Calibration Plot (Fold 0)'\n)\n\n# 15. LightGBM Calibration Curve\ny_proba_lgbm = lgbm_probas_fold0[proba_cols].values\nplot_calibration_curve(\n    y_val_enc, y_proba_lgbm, le.classes_,\n    os.path.join(config['output']['figs_dir'], 'lgbm_calibration.png'),\n    n_bins=10, title='LightGBM - Calibration Plot (Fold 0)'\n)\n\nprint(\"✓ XGBoost visualizations completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# ADDITIONAL COMPREHENSIVE VISUALIZATIONS AND TABLES\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"GENERATING ADDITIONAL VISUALIZATIONS AND TABLES\")\nprint(\"=\"*80)\n\n# 1. Correlation Heatmap\nprint(\"\\n[1/15] Creating correlation heatmap...\")\ncorr_df = plot_correlation_heatmap(\n    df, \n    config['features']['numeric'][:30],  # Top 30 numeric features\n    os.path.join(config['output']['figs_dir'], 'correlation_heatmap.png')\n)\n\n# 2. Class Distribution Per Fold\nprint(\"\\n[2/15] Plotting class distribution per fold...\")\nplot_class_distribution_per_fold(\n    df, target_multi, \n    os.path.join(config['output']['figs_dir'], 'class_distribution_per_fold.png'),\n    n_splits=n_splits\n)\n\n# 3. Categorical Feature Distributions\nprint(\"\\n[3/15] Plotting categorical distributions...\")\nfor cat_col in ['proto', 'service', 'state']:\n    if cat_col in df.columns:\n        plot_categorical_distribution(\n            df, cat_col,\n            os.path.join(config['output']['figs_dir'], f'{cat_col}_distribution.png'),\n            top_n=15\n        )\n\n# 4. LightGBM Metric Progression\nprint(\"\\n[4/15] Plotting LightGBM metric progression...\")\nplot_metric_progression(\n    lgbm_cv_df, 'macro_f1',\n    os.path.join(config['output']['figs_dir'], 'lgbm_metric_progression_f1.png'),\n    model_name='LightGBM'\n)\n\n# 5. XGBoost Metric Progression\nprint(\"\\n[5/15] Plotting XGBoost metric progression...\")\nplot_metric_progression(\n    xgb_cv_df, 'macro_f1',\n    os.path.join(config['output']['figs_dir'], 'xgb_metric_progression_f1.png'),\n    model_name='XGBoost'\n)\n\n# 6. Training Time Comparison\nprint(\"\\n[6/15] Plotting training time comparison...\")\nplot_training_time_comparison(\n    main_results_df,\n    os.path.join(config['output']['figs_dir'], 'training_time_comparison.png')\n)\n\n# 7. Model Comparison Radar Chart\nprint(\"\\n[7/15] Creating model comparison radar chart...\")\nplot_model_comparison_radar(\n    main_results_df,\n    os.path.join(config['output']['figs_dir'], 'model_comparison_radar.png')\n)\n\n# 8. Feature Importance Comparison\nprint(\"\\n[8/15] Creating feature importance comparison...\")\nif len(lgbm_feature_importance) > 0 and len(xgb_feature_importance) > 0:\n    fi_comparison = {\n        'LightGBM': lgbm_fi_df,\n        'XGBoost': xgb_fi_df\n    }\n    plot_feature_importance_comparison(\n        fi_comparison,\n        os.path.join(config['output']['figs_dir'], 'feature_importance_comparison.png'),\n        top_n=20\n    )\n\n# 9. Boxplot Comparison for F1 Scores\nprint(\"\\n[9/15] Creating F1 score boxplot comparison...\")\nf1_comparison = {\n    'LightGBM': lgbm_cv_df['macro_f1'].tolist(),\n    'XGBoost': xgb_cv_df['macro_f1'].tolist()\n}\nplot_boxplot_comparison(\n    f1_comparison,\n    os.path.join(config['output']['figs_dir'], 'model_comparison_boxplot_f1.png'),\n    title='Macro F1 Score Comparison Across 5 Folds',\n    ylabel='Macro F1 Score'\n)\n\nprint(\"\\n✓ Visualizations 1-9 completed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 13.6 Additional Visualizations and Tables",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5 Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 20 features for LightGBM\n",
    "if len(lgbm_feature_importance) > 0:\n",
    "    lgbm_fi_df = pd.read_csv(\n",
    "        os.path.join(config['output']['tables_dir'], 'lgbm_feature_importance.csv')\n",
    "    )\n",
    "    \n",
    "    top_features = lgbm_fi_df.nlargest(20, 'gain_importance')\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['gain_importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Gain Importance')\n",
    "    plt.title('LightGBM - Top 20 Features by Gain')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(config['output']['figs_dir'], 'feature_importance_lgbm.png'),\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (LightGBM):\")\n",
    "    display(lgbm_fi_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Reproducibility Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reproducibility manifest\n",
    "manifest_df = create_reproducibility_manifest(\n",
    "    os.path.join(config['output']['tables_dir'], 'reproducibility_manifest.csv')\n",
    ")\n",
    "display(manifest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNSW-NB15 ANALYSIS - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset: {len(df):,} samples, {len(df.columns)} features\")\n",
    "print(f\"Target classes: {len(le_target.classes_)} ({', '.join(le_target.classes_)})\")\n",
    "print(f\"CV Strategy: {cv_strategy} with {n_splits} folds\")\n",
    "print(f\"\\nModels trained: LightGBM, XGBoost\")\n",
    "print(f\"\\nBest model by Macro F1: \", end=\"\")\n",
    "\n",
    "best_f1 = main_results_df.loc[main_results_df['macro_f1_mean'].idxmax()]\n",
    "print(f\"{best_f1['model']} (F1={best_f1['macro_f1_mean']:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ All results saved to: {config['output']['artifacts_dir']}\")\n",
    "print(f\"  - Tables: {config['output']['tables_dir']}\")\n",
    "print(f\"  - Figures: {config['output']['figs_dir']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps (Optional Extensions)\n",
    "\n",
    "1. **TabTransformer (PyTorch)**: Implement deep learning model for tabular data\n",
    "2. **Hyperparameter Optimization (Optuna)**: Automated HPO for better performance\n",
    "3. **Ensemble Learning**: Combine predictions from multiple models\n",
    "4. **Calibration**: Isotonic/Platt calibration for probability estimates\n",
    "5. **Ablation Studies**: Test impact of SMOTE, focal loss, etc.\n",
    "6. **SHAP Analysis**: Detailed explainability with SHAP values\n",
    "7. **Cross-Dataset Evaluation**: Test on CIC-IDS2017 or other datasets\n",
    "\n",
    "To implement these extensions, uncomment and run the corresponding sections below or create new notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for TabTransformer implementation\n",
    "# See separate notebook: tabtransformer_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for Optuna HPO\n",
    "# See separate notebook: hyperparameter_optimization.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Notebook**\n",
    "\n",
    "For questions or issues, please refer to the project README or contact the research team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}